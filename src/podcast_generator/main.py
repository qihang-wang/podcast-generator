"""
GDELT æ–°é—»æ•°æ®è·å–ä¸»ç¨‹åº
ä» BigQuery è·å– GDELT æ•°æ®å¹¶ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š
"""

import os
import pandas as pd
from datetime import datetime

# å¯¼å…¥ GDELT æ•°æ®è·å–æ¨¡å—
from gdelt_fetcher import fetch_gdelt_data, load_local_data, save_data

# å¯¼å…¥æ•°æ®è§£ææ¨¡å—
from gdelt_parser import process_narrative

# å¯¼å…¥æ–°é—»åˆå¹¶æ¨¡å—
from news_merger import merge_related_news

# å¯¼å…¥æŠ¥å‘Šè§£ææ¨¡å—
from parse_report import (
    parse_report,
    get_report_summary,
    search_reports,
    filter_by_criteria,
)

# å¯¼å…¥ LLM æ–°é—»ç”Ÿæˆæ¨¡å—
from llm_generator import generate_news_from_record, LLMNewsGenerator


# ================= é…ç½®åŒº =================
import pathlib
_SCRIPT_DIR = pathlib.Path(__file__).parent
KEY_PATH = str(_SCRIPT_DIR.parent.parent / 'gdelt_config' / 'my-gdelt-key.json')
PROJECT_ID = 'gdelt-analysis-480906'

# æ–°é—»ç”Ÿæˆè¯­è¨€é…ç½®: "zh" = ä¸­æ–‡, "en" = è‹±æ–‡
NEWS_LANGUAGE = "zh"  # å¯é€‰: "zh" æˆ– "en"

# GDELT æŸ¥è¯¢é…ç½®å·²ç§»è‡³ gdelt_fetcher.py


def print_preview(result_df: pd.DataFrame, offset: int = 0, count: int = 5):
    """
    æ‰“å°æŠ¥å‘Šé¢„è§ˆ
    
    Args:
        result_df: ç»“æœ DataFrame
        offset: é¢„è§ˆè®°å½•èµ·å§‹ä½ç½®
        count: é¢„è§ˆè®°å½•æ•°é‡
    """

    for i, row in result_df.iloc[offset:offset+count].iterrows():
        print(f"\n--- è®°å½• {i+1} ---")
        print(f"ğŸ“Œ æ ‡é¢˜: {row['Title']}")
        print(f"ğŸ“° æ¥æº: {row['Source_Name']}")
        print(f"ğŸ“° æºURL: {row['Source_URL']}")
        print(f"ğŸ• æ—¶é—´: {row['Time']}")
        print(f"ğŸ“ åœ°ç‚¹: {row['Locations']}")
        print(f"ğŸ¢ æœºæ„: {row['Organizations']}")
        print(f"ğŸ‘¤ äººç‰©: {row['Key_Persons']}")
        print(f"ğŸ­ æƒ…æ„Ÿ: {row['Emotions']}")
        print(f"ğŸ“Š åŸºè°ƒ: {row['Tone']}")
        print(f"ğŸ·ï¸ ä¸»é¢˜: {row['Themes']}")
        print(f"ğŸ’¬ å¼•ç”¨:\n{row['Quotes'][:500]}...")
        print(f"ğŸ“ˆ æ•°æ®: {row['Data_Facts']}")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡: {row['Images'][:100]}...")
        print(f"ğŸ“° åŸæ–‡æ‘˜è¦:\n{row['Article_Summary'][:1000]}...")


def analyze_report(filename: str):
    """
    è§£æå¹¶åˆ†æç”Ÿæˆçš„æŠ¥å‘Š
    
    Args:
        filename: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    print("\n" + "="*60)
    print("ğŸ“Š æ­£åœ¨è§£æç”Ÿæˆçš„æŠ¥å‘Š...")
    print("="*60)
    
    try:
        # è§£ææŠ¥å‘Š
        report_result = parse_report(filename)
        
        # è·å–æ‘˜è¦ä¿¡æ¯
        summary = get_report_summary(filename)
        
        print(f"\nğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
        print(f"  - æ–‡ä»¶å: {summary['file_name']}")
        print(f"  - è®°å½•æ€»æ•°: {summary['record_count']}")
        print(f"  - å”¯ä¸€æ¥æºæ•°: {summary['source_count']}")
        
        print(f"\nğŸ­ æƒ…æ„Ÿåˆ†å¸ƒ:")
        for tone, count in summary['tone_stats'].items():
            if count > 0:
                percentage = summary.get('tone_percentages', {}).get(tone, 0)
                print(f"  - {tone}: {count} ({percentage}%)")
        
        print(f"\nğŸ“° ä¸»è¦æ¥æº:")
        for source in summary['top_sources'][:5]:
            print(f"  - {source}")
        
        # æœç´¢ç¤ºä¾‹ï¼šæŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®è¯çš„è®°å½•
        crisis_records = search_reports("crisis", filename)
        if crisis_records:
            print(f"\nğŸ” åŒ…å« 'crisis' å…³é”®è¯çš„è®°å½•: {len(crisis_records)} æ¡")
        
        # æŒ‰æƒ…æ„Ÿç­›é€‰ç¤ºä¾‹
        negative_records = filter_by_criteria(filename, tone="Negative")
        positive_records = filter_by_criteria(filename, tone="Positive")
        print(f"\nğŸ“ˆ æƒ…æ„Ÿç­›é€‰ç»“æœ:")
        print(f"  - è´Ÿé¢æŠ¥é“: {len(negative_records)} æ¡")
        print(f"  - æ­£é¢æŠ¥é“: {len(positive_records)} æ¡")
        
        print("\n" + "="*60)
        print("âœ… æŠ¥å‘Šè§£æå®Œæˆï¼")
        print("="*60)
        
    except Exception as parse_error:
        print(f"âš ï¸ æŠ¥å‘Šè§£ææ—¶å‡ºç°é”™è¯¯: {parse_error}")


def generate_news_with_llm(record: dict, language: str = "zh"):
    """
    ä½¿ç”¨ LLM ç”Ÿæˆæ–°é—»æ–‡æœ¬
    
    Args:
        record: è§£æåçš„æ–°é—»è®°å½•å­—å…¸
        language: è¯­è¨€ä»£ç ï¼Œ"zh" ä¸ºä¸­æ–‡ï¼Œ"en" ä¸ºè‹±æ–‡
    """
    lang_name = "è‹±æ–‡" if language == "en" else "ä¸­æ–‡"
    print("\n" + "="*60)
    print(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨ LLM ç”Ÿæˆ{lang_name}æ–°é—»æ–‡æœ¬...")
    print("="*60)
    
    print(f"\nğŸ“ è¾“å…¥æ•°æ®:")
    print(f"  - æ ‡é¢˜: {record.get('Title')}")
    print(f"  - æ¥æº: {record.get('Source_Name')}")
    print(f"  - åœ°ç‚¹: {record.get('Locations')}")
    print(f"  - ä¸»é¢˜: {record.get('Themes')}")
    
    try:
        news_text = generate_news_from_record(record, language=language)
        return news_text
    except Exception as e:
        error_msg = f"LLM ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(f"\nâš ï¸ {error_msg}")
        return error_msg


def main():
    """ä¸»å‡½æ•°"""
    # æ–¹å¼1: ä» BigQuery è·å–æ•°æ®ï¼ˆæ¶ˆè€—é¢åº¦ï¼Œè°¨æ…ä½¿ç”¨ï¼‰
    # raw_df = fetch_gdelt_data(key_path=KEY_PATH, project_id=PROJECT_ID)
    
    # æ–¹å¼2: ä»æœ¬åœ°æ–‡ä»¶è¯»å–æ•°æ®
    data_dir = _SCRIPT_DIR.parent.parent / '.data'
    raw_path = data_dir / "gdelt_raw_data.csv"
    raw_df = load_local_data(str(raw_path))
    
    if raw_df.empty:
        print("é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶æˆ–æ•°æ®ä¸ºç©º")
        return
    
    try:
        # æ•°æ®ä¿å­˜ç›®å½•
        data_dir = _SCRIPT_DIR.parent.parent / '.data'
        data_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_path = data_dir / "gdelt_raw_data.csv"
        save_data(raw_df, str(raw_path))
        
        # å¤„ç†æ•°æ®
        narratives = raw_df.apply(process_narrative, axis=1).tolist()
        
        # åˆå¹¶ç›¸å…³/é‡å¤çš„æ–°é—»è®°å½•
        merged_narratives = merge_related_news(narratives, similarity_threshold=0.6)
        
        # ä½¿ç”¨åˆå¹¶åçš„æ•°æ®åˆ›å»º DataFrame
        result_df = pd.DataFrame(merged_narratives)
        
        # æ‰“å°é¢„è§ˆ
        print_preview(result_df, offset=0, count=10)
        
        # ä¿å­˜ç»“æœ
        filename = f"gdelt_report_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
        report_path = data_dir / filename
        save_data(result_df, str(report_path))
        
        # è§£ææŠ¥å‘Š
        analyze_report(str(report_path))
        
        # ================= LLM ç”Ÿæˆæ–°é—» =================
        print(f"\nğŸ“ æ–°é—»ç”Ÿæˆè¯­è¨€: {'è‹±æ–‡' if NEWS_LANGUAGE == 'en' else 'ä¸­æ–‡'}")
        if merged_narratives:
            # å–å‰10æ¡åˆå¹¶åçš„æ•°æ®è¿›è¡Œæ–°é—»ç”Ÿæˆ
            news_count = min(10, len(merged_narratives))
            for i, record in enumerate(merged_narratives[0:news_count], 1):
                print(f"\n{'='*60}")
                print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆç¬¬ {i}/{news_count} æ¡æ–°é—»...")
                print(f"{'='*60}")
                generate_news_with_llm(record, language=NEWS_LANGUAGE)
        else:
            print("\nâš ï¸ æ²¡æœ‰å¯ç”¨çš„æ•°æ®è®°å½•")
        
    except Exception as e:
        print(f"æ•°æ®è§£æé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()