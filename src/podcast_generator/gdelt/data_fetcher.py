"""
GDELT æ•°æ®è·å–æ¨¡å—
ä» BigQuery è·å– Event -> Mentions -> GKG å®Œæ•´æ•°æ®æµ

å…¬å¼€æ–¹æ³•ï¼š
    - fetch_gdelt_data: è·å– GDELT æ•°æ®çš„å”¯ä¸€å…¥å£
"""

import os
import pandas as pd
from datetime import datetime
from collections import defaultdict


from .gdelt_service import GDELTQueryService
from .gdelt_mentions import select_best_mentions_per_event


# ========== ç§æœ‰å¸¸é‡ ==========
_GKG_CSV_DIR = os.path.join(os.path.dirname(__file__), "gkg_csv")


def fetch_gdelt_data(
    location_name: str = None,
    country_code: str = None,
    hours_back: int = 24,
    event_limit: int = 100,
    min_confidence: int = 80,
    max_sentence_id: int = 1
):
    """
    è·å– GDELT å®Œæ•´æ•°æ®å¹¶ä¿å­˜åˆ° CSV æ–‡ä»¶
    
    è¿™æ˜¯è·å– GDELT æ•°æ®çš„å”¯ä¸€å…¬å¼€å…¥å£ã€‚è·å–å®Œæˆåè‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ° CSVã€‚
    
    Args:
        location_name: åœ°ç‚¹åç§°ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
        country_code: å›½å®¶ä»£ç ï¼ˆå¦‚ "CH" è¡¨ç¤ºä¸­å›½ï¼‰
        hours_back: æŸ¥è¯¢æœ€è¿‘Nå°æ—¶çš„æ•°æ®ï¼Œé»˜è®¤24å°æ—¶
        event_limit: äº‹ä»¶æ•°é‡é™åˆ¶ï¼Œé»˜è®¤100
        min_confidence: Mentions æœ€å°ç½®ä¿¡åº¦ï¼Œé»˜è®¤80%
        max_sentence_id: å¥å­IDé™åˆ¶ï¼ˆ1=ä»…å¯¼è¯­ï¼‰ï¼Œé»˜è®¤1
        
    Examples:
        # è·å–ä¸­å›½ç›¸å…³äº‹ä»¶
        fetch_gdelt_data(country_code="CH")
        
        # è·å–åŒ—äº¬ç›¸å…³äº‹ä»¶
        fetch_gdelt_data(location_name="Beijing")
    """
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹ GDELT æ•°æ®è·å–")
    print("=" * 80)
    
    service = GDELTQueryService()
    
    # Step 1: è·å–äº‹ä»¶
    print(f"\nğŸ“ æ­¥éª¤ 1/3: æŸ¥è¯¢ Event è¡¨")
    print(f"   å‚æ•°: location={location_name or 'ä¸é™'}, country={country_code or 'ä¸é™'}, hours={hours_back}h")
    
    events = service.query_events_by_location(
        location_name=location_name,
        country_code=country_code,
        hours_back=hours_back,
        limit=event_limit,
        print_progress=True
    )
    
    if not events:
        print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº‹ä»¶")
        return
    
    print(f"âœ“ æ‰¾åˆ° {len(events)} ä¸ªäº‹ä»¶")
    
    # Step 2: è·å– Mentions
    print(f"\nğŸ“° æ­¥éª¤ 2/3: æŸ¥è¯¢ Mentions è¡¨")
    print(f"   å‚æ•°: Confidence>={min_confidence}%, SentenceID<={max_sentence_id}")
    
    event_ids = [e.global_event_id for e in events]
    all_mentions = service.query_mentions_by_event_ids(
        event_ids=event_ids,
        min_confidence=min_confidence,
        sentence_id=max_sentence_id,
        print_progress=True
    )
    
    if not all_mentions:
        print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æŠ¥é“")
        return
    
    # æ‰“å°äº‹ä»¶æ±‡æ€»
    _print_event_summary(events, all_mentions)
    
    # ç­›é€‰æœ€ä½³æŠ¥é“
    all_mentions = select_best_mentions_per_event(all_mentions)
    
    # Step 3: è·å– GKG æ•°æ®
    print(f"\nğŸ” æ­¥éª¤ 3/3: æŸ¥è¯¢ GKG è¡¨")
    
    mention_urls = [m.mention_identifier for m in all_mentions if m.mention_identifier]
    if not mention_urls:
        print("âš ï¸ æ— æœ‰æ•ˆ URL")
        return
    
    gkg_df = service.query_gkg_raw(mention_urls, print_progress=True)
    
    if gkg_df.empty:
        print("âš ï¸ æœªè·å–åˆ° GKG æ•°æ®")
        return
    
    print(f"âœ“ è·å–åˆ° {len(gkg_df)} æ¡ GKG æ•°æ®")
    
    # ä¿å­˜åˆ° CSV
    _save_to_csv(gkg_df, country_code)
    
    # å®Œæˆ
    print("\n" + "=" * 80)
    print(f"âœ… å®Œæˆï¼{len(events)} ä¸ªäº‹ä»¶ï¼Œ{len(gkg_df)} ç¯‡æ–‡ç« ")
    print("=" * 80 + "\n")



# ========== ç§æœ‰æ–¹æ³• ==========

def _save_to_csv(gkg_df: pd.DataFrame, country_code: str = None) -> str:
    """ä¿å­˜ DataFrame åˆ° CSV æ–‡ä»¶ï¼ŒæŒ‰ country_code å‘½å"""
    os.makedirs(_GKG_CSV_DIR, exist_ok=True)
    
    # æ–‡ä»¶åæŒ‰ country_code å‘½åï¼Œå¦‚ CH.csv, US.csv
    if country_code:
        filename = f"{country_code.upper()}.csv"
    else:
        filename = "default.csv"
    
    file_path = os.path.join(_GKG_CSV_DIR, filename)
    gkg_df.to_csv(file_path, index=False, encoding='utf-8-sig')
    print(f"âœ“ æ•°æ®å·²ä¿å­˜: {filename} ({len(gkg_df)} æ¡)")
    
    return file_path



def _print_event_summary(events, mentions):
    """æ‰“å°äº‹ä»¶æ±‡æ€»ä¿¡æ¯"""
    events_dict = {e.global_event_id: e for e in events}
    mentions_by_event = defaultdict(list)
    
    for mention in mentions:
        mentions_by_event[mention.global_event_id].append(mention)
    
    print(f"\nğŸ“Š {len(mentions)} æ¡æŠ¥é“æŒ‰äº‹ä»¶åˆ†ç»„ï¼š")
    for event_id, event_mentions in mentions_by_event.items():
        event = events_dict.get(event_id)
        if event:
            print(f"   EventID {event_id} | "
                  f"QuadClass={event.quad_class} | "
                  f"EventCode={event.event_code} | "
                  f"{event.action_geo.full_name} | "
                  f"{event.actor1.name or event.actor1.code} â†’ "
                  f"{event.actor2.name or event.actor2.code} | "
                  f"{len(event_mentions)} æ¡")
