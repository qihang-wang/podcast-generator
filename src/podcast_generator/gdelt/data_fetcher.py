"""
GDELT æ•°æ®è·å–æ¨¡å—
ä» BigQuery è·å– Event -> Mentions -> GKG å®Œæ•´æ•°æ®æµ

å…¬å¼€æ–¹æ³•ï¼š
    - fetch_gdelt_data: è·å– GDELT æ•°æ®çš„å”¯ä¸€å…¥å£
"""

import os
import pandas as pd
from datetime import datetime
from collections import defaultdict


from .gdelt_service import GDELTQueryService
from .gdelt_mentions import select_best_mentions_per_event


# ========== ç§æœ‰å¸¸é‡ ==========
_GDELT_CSV_DIR = os.path.join(os.path.dirname(__file__), "gdelt_csv")


def fetch_gdelt_data(
    location_name: str = None,
    country_code: str = None,
    hours_back: int = 24,
    event_limit: int = 100,
    min_confidence: int = 80,
    max_sentence_id: int = 1
):
    """
    è·å– GDELT å®Œæ•´æ•°æ®å¹¶ä¿å­˜åˆ° CSV æ–‡ä»¶
    
    è¿™æ˜¯è·å– GDELT æ•°æ®çš„å”¯ä¸€å…¬å¼€å…¥å£ã€‚è·å–å®Œæˆåè‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ° CSVã€‚
    
    Args:
        location_name: åœ°ç‚¹åç§°ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
        country_code: å›½å®¶ä»£ç ï¼ˆå¦‚ "CH" è¡¨ç¤ºä¸­å›½ï¼‰
        hours_back: æŸ¥è¯¢æœ€è¿‘Nå°æ—¶çš„æ•°æ®ï¼Œé»˜è®¤24å°æ—¶
        event_limit: äº‹ä»¶æ•°é‡é™åˆ¶ï¼Œé»˜è®¤100
        min_confidence: Mentions æœ€å°ç½®ä¿¡åº¦ï¼Œé»˜è®¤80%
        max_sentence_id: å¥å­IDé™åˆ¶ï¼ˆ1=ä»…å¯¼è¯­ï¼‰ï¼Œé»˜è®¤1
        
    Examples:
        # è·å–ä¸­å›½ç›¸å…³äº‹ä»¶
        fetch_gdelt_data(country_code="CH")
        
        # è·å–åŒ—äº¬ç›¸å…³äº‹ä»¶
        fetch_gdelt_data(location_name="Beijing")
    """
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹ GDELT æ•°æ®è·å–")
    print("=" * 80)
    
    service = GDELTQueryService()
    
    # Step 1: è·å–äº‹ä»¶
    print(f"\nğŸ“ æ­¥éª¤ 1/3: æŸ¥è¯¢ Event è¡¨")
    print(f"   å‚æ•°: location={location_name or 'ä¸é™'}, country={country_code or 'ä¸é™'}, hours={hours_back}h")
    
    events = service.query_events_by_location(
        location_name=location_name,
        country_code=country_code,
        hours_back=hours_back,
        limit=event_limit,
        print_progress=True
    )
    
    if not events:
        print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº‹ä»¶")
        return
    
    print(f"âœ“ æ‰¾åˆ° {len(events)} ä¸ªäº‹ä»¶")
    
    # Step 2: è·å– Mentions
    print(f"\nğŸ“° æ­¥éª¤ 2/3: æŸ¥è¯¢ Mentions è¡¨")
    print(f"   å‚æ•°: Confidence>={min_confidence}%, SentenceID<={max_sentence_id}")
    
    event_ids = [e.global_event_id for e in events]
    all_mentions = service.query_mentions_by_event_ids(
        event_ids=event_ids,
        min_confidence=min_confidence,
        sentence_id=max_sentence_id,
        print_progress=True
    )
    
    if not all_mentions:
        print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æŠ¥é“")
        return
    
    # æ‰“å°äº‹ä»¶æ±‡æ€»
    _print_event_summary(events, all_mentions)
    
    # ç­›é€‰æœ€ä½³æŠ¥é“
    all_mentions = select_best_mentions_per_event(all_mentions)
    
    # ç­›é€‰å‡ºä¸ mentions ç›¸å…³çš„äº‹ä»¶
    related_event_ids = set(m.global_event_id for m in all_mentions)
    related_events = [e for e in events if e.global_event_id in related_event_ids]
    print(f"âœ“ ç­›é€‰å‡º {len(related_events)} ä¸ªç›¸å…³äº‹ä»¶")
    
    # Step 3: è·å– GKG æ•°æ®
    print(f"\nğŸ” æ­¥éª¤ 3/3: æŸ¥è¯¢ GKG è¡¨")
    
    mention_urls = [m.mention_identifier for m in all_mentions if m.mention_identifier]
    if not mention_urls:
        print("âš ï¸ æ— æœ‰æ•ˆ URL")
        return
    
    gkg_df = service.query_gkg_raw(mention_urls, print_progress=True)
    
    if gkg_df.empty:
        print("âš ï¸ æœªè·å–åˆ° GKG æ•°æ®")
        return
    
    # å»ºç«‹ URL -> EventID æ˜ å°„ï¼Œæ·»åŠ åˆ° GKG DataFrame
    url_to_event = {m.mention_identifier: m.global_event_id for m in all_mentions}
    gkg_df['event_id'] = gkg_df['DocumentIdentifier'].map(url_to_event)
    
    print(f"âœ“ è·å–åˆ° {len(gkg_df)} æ¡ GKG æ•°æ®ï¼Œå·²å…³è” event_id")
    
    # ä¿å­˜åˆ° CSV
    _save_gkg_to_csv(gkg_df, country_code)
    _save_events_to_csv(related_events, country_code)

    
    # å®Œæˆ
    print("\n" + "=" * 80)
    print(f"âœ… å®Œæˆï¼{len(related_events)} ä¸ªäº‹ä»¶ï¼Œ{len(gkg_df)} ç¯‡æ–‡ç« ")
    print("=" * 80 + "\n")




# ========== ç§æœ‰æ–¹æ³• ==========

def _save_gkg_to_csv(gkg_df: pd.DataFrame, country_code: str = None) -> str:
    """ä¿å­˜ GKG DataFrame åˆ° CSV æ–‡ä»¶"""
    os.makedirs(_GDELT_CSV_DIR, exist_ok=True)
    
    if country_code:
        filename = f"{country_code.upper()}_gkg.csv"
    else:
        filename = "default_gkg.csv"
    
    file_path = os.path.join(_GDELT_CSV_DIR, filename)
    gkg_df.to_csv(file_path, index=False, encoding='utf-8-sig')
    print(f"âœ“ GKG æ•°æ®å·²ä¿å­˜: {filename} ({len(gkg_df)} æ¡)")
    
    return file_path



def _save_events_to_csv(events, country_code: str = None) -> str:
    """ä¿å­˜ EventModel åˆ—è¡¨åˆ° CSV æ–‡ä»¶ï¼ˆä½¿ç”¨ BigQuery åˆ—åä»¥ä¾¿å¤ç”¨åŠ è½½å‡½æ•°ï¼‰"""
    os.makedirs(_GDELT_CSV_DIR, exist_ok=True)
    
    if country_code:
        filename = f"{country_code.upper()}_event.csv"
    else:
        filename = "default_event.csv"
    
    # å°† EventModel è½¬æ¢ä¸º DataFrameï¼ˆä½¿ç”¨ BigQuery åŸå§‹åˆ—åï¼‰
    rows = []
    for e in events:
        rows.append({
            'GLOBALEVENTID': e.global_event_id,
            'SQLDATE': e.sql_date,
            'Actor1Code': e.actor1.code,
            'Actor1Name': e.actor1.name,
            'Actor1CountryCode': e.actor1.country_code,
            'Actor1Type1Code': e.actor1.type1_code,
            'Actor2Code': e.actor2.code,
            'Actor2Name': e.actor2.name,
            'Actor2CountryCode': e.actor2.country_code,
            'Actor2Type1Code': e.actor2.type1_code,
            'EventCode': e.event_code,
            'EventBaseCode': e.event_base_code,
            'EventRootCode': e.event_root_code,
            'QuadClass': e.quad_class,
            'GoldsteinScale': e.goldstein_scale,
            'NumMentions': e.num_mentions,
            'NumSources': e.num_sources,
            'NumArticles': e.num_articles,
            'AvgTone': e.avg_tone,
            'ActionGeo_Type': e.action_geo.geo_type,
            'ActionGeo_FullName': e.action_geo.full_name,
            'ActionGeo_CountryCode': e.action_geo.country_code,
            'ActionGeo_ADM1Code': e.action_geo.adm1_code,
            'ActionGeo_Lat': e.action_geo.lat,
            'ActionGeo_Long': e.action_geo.long,
            'ActionGeo_FeatureID': e.action_geo.feature_id,
            'SOURCEURL': e.source_url,
            'DATEADDED': e.date_added,
        })
    
    df = pd.DataFrame(rows)
    file_path = os.path.join(_GDELT_CSV_DIR, filename)
    df.to_csv(file_path, index=False, encoding='utf-8-sig')
    print(f"âœ“ Event æ•°æ®å·²ä¿å­˜: {filename} ({len(rows)} æ¡)")
    
    return file_path



def _print_event_summary(events, mentions):
    """æ‰“å°äº‹ä»¶æ±‡æ€»ä¿¡æ¯"""
    events_dict = {e.global_event_id: e for e in events}
    mentions_by_event = defaultdict(list)
    
    for mention in mentions:
        mentions_by_event[mention.global_event_id].append(mention)
    
    print(f"\nğŸ“Š {len(mentions)} æ¡æŠ¥é“æŒ‰äº‹ä»¶åˆ†ç»„ï¼š")
    for event_id, event_mentions in mentions_by_event.items():
        event = events_dict.get(event_id)
        if event:
            print(f"   EventID {event_id} | "
                  f"QuadClass={event.quad_class} | "
                  f"EventCode={event.event_code} | "
                  f"{event.action_geo.full_name} | "
                  f"{event.actor1.name or event.actor1.code} â†’ "
                  f"{event.actor2.name or event.actor2.code} | "
                  f"{len(event_mentions)} æ¡")
