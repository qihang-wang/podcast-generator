"""
å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
ä½¿ç”¨ APScheduler å®ç°åå°å®šæ—¶ä»»åŠ¡

ä»»åŠ¡åˆ—è¡¨ï¼ˆæ¯å¤©å‡Œæ™¨0ç‚¹é¡ºåºæ‰§è¡Œï¼‰ï¼š
1. æ•°æ®æ¸…ç†ï¼šæ¸…ç†è¿‡æœŸæ•°æ®
2. æ•°æ®é¢„çƒ­ï¼šé¢„çƒ­å¸¸ç”¨å›½å®¶çš„æ•°æ®
"""

import logging
import os
from datetime import datetime, timedelta
from contextlib import asynccontextmanager

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
scheduler = AsyncIOScheduler()

# é¢„çƒ­çš„å›½å®¶ä»£ç åˆ—è¡¨ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼‰
DEFAULT_PREHEAT_COUNTRIES = ["CH", "US"]


def preheat_data():
    """
    é¢„çƒ­æ•°æ®ï¼šé¢„å…ˆè·å–å¸¸ç”¨å›½å®¶æ˜¨å¤©çš„æ•°æ®
    
    é…ç½®ç¯å¢ƒå˜é‡ï¼š
    - PREHEAT_COUNTRIES: é¢„çƒ­çš„å›½å®¶ä»£ç ï¼Œé€—å·åˆ†éš”ï¼ˆé»˜è®¤ "CH,US"ï¼‰
    - PREHEAT_DAYS: é¢„çƒ­çš„å¤©æ•°ï¼ˆé»˜è®¤ 1ï¼Œå³æ˜¨å¤©ï¼‰
    """
    try:
        from podcast_generator.database import ArticleRepository
        from podcast_generator.api.routes.articles_helpers import (
            get_days_list, check_day_cached, fetch_day_data
        )
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            logging.warning("âš ï¸ æ•°æ®åº“ä¸å¯ç”¨ï¼Œè·³è¿‡é¢„çƒ­ä»»åŠ¡")
            return
        
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        countries_str = os.getenv("PREHEAT_COUNTRIES", ",".join(DEFAULT_PREHEAT_COUNTRIES))
        countries = [c.strip().upper() for c in countries_str.split(",") if c.strip()]
        days = int(os.getenv("PREHEAT_DAYS", "1"))
        
        logging.info(f"ğŸ”¥ [å®šæ—¶ä»»åŠ¡] å¼€å§‹æ•°æ®é¢„çƒ­: å›½å®¶={countries}, å¤©æ•°={days}")
        
        # è·å–éœ€è¦é¢„çƒ­çš„æ—¥æœŸ
        dates = get_days_list(days)
        
        total_fetched = 0
        for country in countries:
            for date in dates:
                date_str = date.strftime("%Y-%m-%d")
                
                if check_day_cached(repo, country, date):
                    logging.debug(f"âœ“ {country} {date_str} å·²æœ‰ç¼“å­˜ï¼Œè·³è¿‡")
                else:
                    logging.info(f"ğŸ“¥ é¢„çƒ­ {country} {date_str}...")
                    fetch_day_data(country, date)
                    total_fetched += 1
        
        if total_fetched > 0:
            logging.info(f"âœ… [å®šæ—¶ä»»åŠ¡] é¢„çƒ­å®Œæˆï¼è·å–äº† {total_fetched} å¤©çš„æ•°æ®")
        else:
            logging.info(f"âœ… [å®šæ—¶ä»»åŠ¡] é¢„çƒ­å®Œæˆï¼æ‰€æœ‰æ•°æ®å·²æ˜¯æœ€æ–°")
            
    except Exception as e:
        logging.error(f"âŒ [å®šæ—¶ä»»åŠ¡] é¢„çƒ­å¤±è´¥: {e}")


def cleanup_old_articles():
    """
    æ¸…ç†è¿‡æœŸæ–‡ç« æ•°æ®
    
    é»˜è®¤æ¸…ç†è¶…è¿‡ 7 å¤©çš„æ•°æ®ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ CLEANUP_DAYS é…ç½®
    """
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            logging.warning("âš ï¸ æ•°æ®åº“ä¸å¯ç”¨ï¼Œè·³è¿‡æ¸…ç†ä»»åŠ¡")
            return
        
        # ä»ç¯å¢ƒå˜é‡è·å–ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤ 7 å¤©
        days = int(os.getenv("CLEANUP_DAYS", "7"))
        
        logging.info(f"ğŸ§¹ [å®šæ—¶ä»»åŠ¡] å¼€å§‹æ¸…ç†è¶…è¿‡ {days} å¤©çš„æ•°æ®...")
        
        # è·å–æ¸…ç†å‰çš„ç»Ÿè®¡
        stats_before = repo.get_storage_stats()
        
        # æ‰§è¡Œæ¸…ç†
        deleted = repo.cleanup_old_articles(days=days)
        
        # è·å–æ¸…ç†åçš„ç»Ÿè®¡
        stats_after = repo.get_storage_stats()
        
        logging.info(
            f"âœ… [å®šæ—¶ä»»åŠ¡] æ¸…ç†å®Œæˆï¼\n"
            f"   åˆ é™¤: {deleted} æ¡\n"
            f"   å‰©ä½™: {stats_after['total_articles']} æ¡\n"
            f"   å­˜å‚¨: {stats_after['estimated_size_mb']:.2f} MB ({stats_after['usage_percent']:.1f}%)"
        )
        
        # å¦‚æœä½¿ç”¨ç‡ä»ç„¶è¾ƒé«˜ï¼Œå‘å‡ºè­¦å‘Š
        if stats_after['warning']:
            logging.warning(stats_after['warning'])
            
    except Exception as e:
        logging.error(f"âŒ [å®šæ—¶ä»»åŠ¡] æ¸…ç†å¤±è´¥: {e}")


def daily_maintenance():
    """
    æ¯æ—¥ç»´æŠ¤ä»»åŠ¡ï¼ˆå‡Œæ™¨0ç‚¹æ‰§è¡Œï¼‰
    
    æ‰§è¡Œé¡ºåºï¼š
    1. å…ˆæ¸…ç†è¿‡æœŸæ•°æ®ï¼ˆè…¾å‡ºç©ºé—´ï¼‰
    2. å†é¢„çƒ­æ–°æ•°æ®
    """
    logging.info("ğŸŒ™ [å®šæ—¶ä»»åŠ¡] å¼€å§‹æ¯æ—¥ç»´æŠ¤...")
    
    # 1. æ¸…ç†è¿‡æœŸæ•°æ®
    cleanup_old_articles()
    
    # 2. é¢„çƒ­æ–°æ•°æ®
    preheat_data()
    
    logging.info("ğŸŒ… [å®šæ—¶ä»»åŠ¡] æ¯æ—¥ç»´æŠ¤å®Œæˆï¼")


def setup_scheduler():
    """
    é…ç½®å®šæ—¶ä»»åŠ¡
    
    ç¯å¢ƒå˜é‡é…ç½®ï¼š
    - MAINTENANCE_HOUR: æ¯æ—¥ç»´æŠ¤ä»»åŠ¡æ‰§è¡Œçš„å°æ—¶ï¼ˆé»˜è®¤ 0ï¼Œå³å‡Œæ™¨0ç‚¹ï¼‰
    - MAINTENANCE_MINUTE: æ¯æ—¥ç»´æŠ¤ä»»åŠ¡æ‰§è¡Œçš„åˆ†é’Ÿï¼ˆé»˜è®¤ 0ï¼‰
    - CLEANUP_DAYS: ä¿ç•™çš„å¤©æ•°ï¼ˆé»˜è®¤ 7ï¼‰
    - PREHEAT_COUNTRIES: é¢„çƒ­çš„å›½å®¶ä»£ç ï¼ˆé»˜è®¤ "CH,US"ï¼‰
    - PREHEAT_DAYS: é¢„çƒ­çš„å¤©æ•°ï¼ˆé»˜è®¤ 1ï¼‰
    """
    # ç»´æŠ¤ä»»åŠ¡é…ç½®
    hour = int(os.getenv("MAINTENANCE_HOUR", "0"))
    minute = int(os.getenv("MAINTENANCE_MINUTE", "0"))
    
    # æ·»åŠ æ¯æ—¥ç»´æŠ¤ä»»åŠ¡ - å‡Œæ™¨0ç‚¹æ‰§è¡Œ
    scheduler.add_job(
        daily_maintenance,
        CronTrigger(hour=hour, minute=minute),
        id="daily_maintenance",
        name="æ¯æ—¥ç»´æŠ¤ï¼ˆæ¸…ç†+é¢„çƒ­ï¼‰",
        replace_existing=True
    )
    
    logging.info(f"ğŸ“… å®šæ—¶ä»»åŠ¡å·²é…ç½®: æ¯å¤© {hour:02d}:{minute:02d} æ‰§è¡Œæ¯æ—¥ç»´æŠ¤ï¼ˆæ¸…ç†+é¢„çƒ­ï¼‰")


def start_scheduler():
    """å¯åŠ¨è°ƒåº¦å™¨"""
    if not scheduler.running:
        setup_scheduler()
        scheduler.start()
        logging.info("âœ… å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")


def stop_scheduler():
    """åœæ­¢è°ƒåº¦å™¨"""
    if scheduler.running:
        scheduler.shutdown()
        logging.info("â¹ï¸ å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")


@asynccontextmanager
async def lifespan_scheduler(app):
    """
    FastAPI lifespan ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    ç”¨äºåœ¨åº”ç”¨å¯åŠ¨/å…³é—­æ—¶ç®¡ç†è°ƒåº¦å™¨
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    ```python
    from podcast_generator.api.scheduler import lifespan_scheduler
    
    app = FastAPI(lifespan=lifespan_scheduler)
    ```
    """
    # å¯åŠ¨æ—¶
    start_scheduler()
    yield
    # å…³é—­æ—¶
    stop_scheduler()
