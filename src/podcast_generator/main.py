"""
GDELT æ–°é—»æ•°æ®è·å–ä¸»ç¨‹åº
ä» BigQuery è·å– GDELT æ•°æ®å¹¶ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š
"""

import os
import pandas as pd
from datetime import datetime

# å¯¼å…¥ GDELT æ•°æ®è·å–æ¨¡å—
from gdelt_fetcher import fetch_gdelt_data, load_local_data, save_data

# å¯¼å…¥æ•°æ®è§£ææ¨¡å—
from gdelt_parser import process_narrative

# å¯¼å…¥æ–°é—»åˆå¹¶æ¨¡å—
from news_merger import merge_related_news

# å¯¼å…¥æŠ¥å‘Šè§£ææ¨¡å—
from parse_report import (
    parse_report,
    get_report_summary,
    search_reports,
    filter_by_criteria,
)

# å¯¼å…¥ LLM æ–°é—»ç”Ÿæˆæ¨¡å—
from llm_generator import generate_news_from_record, LLMNewsGenerator


# ================= é…ç½®åŒº =================
import pathlib
_SCRIPT_DIR = pathlib.Path(__file__).parent
KEY_PATH = str(_SCRIPT_DIR.parent.parent / 'gdelt_config' / 'my-gdelt-key.json')
PROJECT_ID = 'gdelt-analysis-480906'

# æ–°é—»ç”ŸæˆèŒƒå›´é…ç½®ï¼ˆæ”¯æŒåˆ†æ‰¹å¤„ç†ï¼‰
NEWS_START_INDEX = 0   # èµ·å§‹ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
NEWS_END_INDEX = 5    # ç»“æŸç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰


def process_and_generate(record: dict, index: int, total: int) -> dict:
    """
    æ‰“å°å•æ¡æ•°æ®é¢„è§ˆå¹¶ç”ŸæˆåŒè¯­æ–°é—»
    å°†é¢„è§ˆå’Œç”Ÿæˆæ”¾åœ¨ä¸€èµ·ï¼Œæ›´ç›´è§‚
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“° ç¬¬ {index}/{total} æ¡æ–°é—»")
    print(f"{'='*60}")
    
    # === å®Œæ•´æ•°æ®é¢„è§ˆ ===
    print(f"\nğŸ“‹ åŸå§‹æ•°æ®:")
    print(f"  ğŸ“Œ æ ‡é¢˜: {record.get('Title')}")
    print(f"  ğŸ“° æ¥æº: {record.get('Source_Name')}")
    print(f"  ï¿½ æºURL: {record.get('Source_URL')}")
    print(f"  ğŸ• æ—¶é—´: {record.get('Time')}")
    print(f"  ï¿½ğŸ“ åœ°ç‚¹: {record.get('Locations')}")
    print(f"  ğŸ¢ æœºæ„: {record.get('Organizations')}")
    print(f"  ğŸ‘¤ äººç‰©: {record.get('Key_Persons')}")
    print(f"  ğŸ­ æƒ…æ„Ÿ: {record.get('Emotions')}")
    print(f"  ğŸ“Š åŸºè°ƒ: {record.get('Tone')}")
    print(f"  ğŸ·ï¸ ä¸»é¢˜: {record.get('Themes')}")
    
    quotes = record.get('Quotes', '')
    quotes = quotes[:500] if len(str(quotes)) > 500 else quotes
    print(f"  ğŸ’¬ å¼•ç”¨:\n{quotes}...")
    
    print(f"  ğŸ“ˆ æ•°æ®: {record.get('Data_Facts')}")
    
    images = record.get('Images', '')
    images = images[:100] if len(str(images)) > 100 else images
    print(f"  ğŸ–¼ï¸ å›¾ç‰‡: {images}...")
    
    summary = record.get('Article_Summary', '')
    summary = summary[:1000] if len(str(summary)) > 1000 else summary
    print(f"  ğŸ“° åŸæ–‡æ‘˜è¦:\n{summary}...")
    
    # === ç”Ÿæˆè‹±æ–‡æ–°é—» ===
    print(f"\nğŸ”¤ ç”Ÿæˆè‹±æ–‡æ–°é—»...")
    try:
        english_news = generate_news_from_record(record, language="en")
        print(f"\nğŸ“° English News:")
        print("-" * 60)
        print(english_news)
        print("-" * 60)
    except Exception as e:
        english_news = f"[Error] {str(e)}"
        print(f"  âš ï¸ è‹±æ–‡ç”Ÿæˆå¤±è´¥: {e}")
    
    # === ç”Ÿæˆä¸­æ–‡æ–°é—» ===
    print(f"\nğŸ”¤ ç”Ÿæˆä¸­æ–‡æ–°é—»...")
    try:
        chinese_news = generate_news_from_record(record, language="zh")
        print(f"\nğŸ“° ä¸­æ–‡æ–°é—»:")
        print("-" * 60)
        print(chinese_news)
        print("-" * 60)
    except Exception as e:
        chinese_news = f"[Error] {str(e)}"
        print(f"  âš ï¸ ä¸­æ–‡ç”Ÿæˆå¤±è´¥: {e}")
    
    return {
        'title': record.get('Title'),
        'source': record.get('Source_Name'),
        'english': english_news,
        'chinese': chinese_news
    }


def main():
    """ä¸»å‡½æ•°"""
    data_dir = _SCRIPT_DIR.parent.parent / '.data'
    raw_path = data_dir / "gdelt_raw_data.csv"
    raw_df = load_local_data(str(raw_path))
    
    if raw_df.empty:
        print("é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶æˆ–æ•°æ®ä¸ºç©º")
        return
    
    try:
        data_dir.mkdir(exist_ok=True)
        save_data(raw_df, str(raw_path))
        
        # å¤„ç†å’Œåˆå¹¶æ•°æ®
        narratives = raw_df.apply(process_narrative, axis=1).tolist()
        merged_narratives = merge_related_news(narratives, similarity_threshold=0.6)
        
        # ä¿å­˜æŠ¥å‘Š
        result_df = pd.DataFrame(merged_narratives)
        filename = f"gdelt_report_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
        report_path = data_dir / filename
        save_data(result_df, str(report_path))
        
        # ================= LLM ç”ŸæˆåŒè¯­æ–°é—» =================
        total_count = len(merged_narratives)
        start_idx = min(NEWS_START_INDEX, total_count)
        end_idx = min(NEWS_END_INDEX, total_count)
        news_to_generate = merged_narratives[start_idx:end_idx]
        
        print(f"\n{'='*60}")
        print(f"ğŸ¤– å¼€å§‹ç”ŸæˆåŒè¯­æ–°é—»")
        print(f"ğŸ“Š æ€»è®°å½•: {total_count} æ¡ï¼Œç”ŸæˆèŒƒå›´: [{start_idx}, {end_idx})")
        print(f"{'='*60}")
        
        if news_to_generate:
            all_news = []
            for i, record in enumerate(news_to_generate, 1):
                result = process_and_generate(record, i, len(news_to_generate))
                all_news.append(result)
            
            print(f"\n{'='*60}")
            print(f"âœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(all_news)} æ¡åŒè¯­æ–°é—»")
            print(f"{'='*60}")
        else:
            print("\nâš ï¸ æ²¡æœ‰å¯ç”¨çš„æ•°æ®")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
