"""
æ–‡ç« æ•°æ® API è·¯ç”±
é‡‡ç”¨ç®€å•çš„æŒ‰å¤©ç¼“å­˜ç­–ç•¥
"""

from fastapi import APIRouter, Query, HTTPException
from datetime import datetime
import logging
import uuid

from .articles_helpers import (
    get_day_range,
    datetime_to_int,
    get_days_list,
    check_day_cached,
    fetch_day_data_with_lock
)
from podcast_generator.api.response import success_response, error_response, ErrorCode

router = APIRouter(prefix="/api/articles", tags=["æ–‡ç« æ•°æ®"])


@router.get("/")
async def get_articles(
    country_code: str = Query("CH", description="å›½å®¶ä»£ç  (FIPS 10-4)"),
    days: int = Query(1, ge=0, le=7, description="è·å–æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆ0-7å¤©ï¼Œ0è¡¨ç¤ºä¸è·å–å†å²æ•°æ®ï¼‰"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡")
):
    """
    è·å–æŒ‡å®šå›½å®¶çš„æ–‡ç« æ•°æ®
    
    é‡‡ç”¨æŒ‰å¤©ç¼“å­˜ç­–ç•¥ï¼š
    - æ¯æ¬¡è¯·æ±‚ä»¥"å®Œæ•´å¤©"ä¸ºå•ä½ï¼ˆ0ç‚¹-24ç‚¹ï¼‰
    - å·²è·å–çš„å¤©æ•°ä¼šè¢«ç¼“å­˜ï¼Œä¸‹æ¬¡è¯·æ±‚ç›´æ¥å‘½ä¸­
    - åªè·å–ç¼ºå¤±çš„å¤©æ•°æ•°æ®
    - å¹¶å‘è¯·æ±‚ä¼šè‡ªåŠ¨åŠ é”ï¼Œé¿å…é‡å¤æŸ¥è¯¢ BigQuery
    
    å‚æ•°ï¼š
    - **country_code**: å›½å®¶ä»£ç ï¼Œå¦‚ "CH"=ä¸­å›½, "US"=ç¾å›½
    - **days**: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆ0-7å¤©ï¼Œä¸å«ä»Šå¤©ï¼‰
    - **page**: é¡µç ï¼ˆé»˜è®¤1ï¼‰
    - **page_size**: æ¯é¡µæ•°é‡ï¼ˆé»˜è®¤20ï¼‰
    """
    request_id = str(uuid.uuid4())[:8]
    start_time = datetime.now()
    
    logging.info(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚ [{request_id}]: country={country_code}, days={days}, page={page}, page_size={page_size}")
    
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            logging.error(f"âŒ [{request_id}] æ•°æ®åº“ä¸å¯ç”¨")
            return error_response(
                code=ErrorCode.DATABASE_UNAVAILABLE,
                message="æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ Supabase é…ç½®",
                request_id=request_id
            )
        
        # days=0 æ—¶è¿”å›ç©ºç»“æœ
        if days == 0:
            logging.info(f"âœ… [{request_id}] days=0ï¼Œè¿”å›ç©ºç»“æœ")
            return success_response(
                data={
                    "articles": [],
                    "pagination": {
                        "total": 0,
                        "page": page,
                        "page_size": page_size,
                        "total_pages": 0
                    }
                },
                request_id=request_id,
                source="database",
                cache_hit=True,
                cached_days=0,
                fetched_days=0
            )
        
        # è·å–éœ€è¦æŸ¥è¯¢çš„æ—¥æœŸåˆ—è¡¨
        dates = get_days_list(days)
        date_range = f"{dates[0].strftime('%Y-%m-%d')} ~ {dates[-1].strftime('%Y-%m-%d')}" if dates else "æ— "
        logging.info(f"ğŸ“… [{request_id}] æŸ¥è¯¢æ—¥æœŸèŒƒå›´: {date_range}")
        
        # æ£€æŸ¥å¹¶è·å–ç¼ºå¤±çš„å¤©æ•°æ®ï¼ˆå¸¦é”ï¼Œé˜²æ­¢å¹¶å‘é‡å¤æŸ¥è¯¢ï¼‰
        cached_days = 0
        fetched_days = 0
        
        for date in dates:
            date_str = date.strftime("%Y-%m-%d")
            
            if check_day_cached(repo, country_code, date):
                logging.info(f"âœ“ [{request_id}] {date_str} å·²ç¼“å­˜")
                cached_days += 1
            else:
                logging.info(f"â—‹ [{request_id}] {date_str} æœªç¼“å­˜ï¼Œå¼€å§‹ä» BigQuery è·å–...")
                # ä½¿ç”¨å¸¦é”çš„ç‰ˆæœ¬ï¼Œé˜²æ­¢å¹¶å‘è¯·æ±‚é‡å¤æŸ¥è¯¢
                actually_fetched = await fetch_day_data_with_lock(repo, country_code, date)
                if actually_fetched:
                    logging.info(f"âœ… [{request_id}] {date_str} è·å–å®Œæˆ")
                    fetched_days += 1
                else:
                    # é”åæ£€æŸ¥å‘ç°ç¼“å­˜å·²ç”±å…¶ä»–è¯·æ±‚å¡«å……
                    logging.info(f"ğŸ”’ [{request_id}] {date_str} ç”±å…¶ä»–è¯·æ±‚å¡«å……")
                    cached_days += 1
        
        # è®¡ç®—æ•´ä¸ªæ—¶é—´èŒƒå›´
        if not dates:
            logging.info(f"âœ… [{request_id}] æ— æ—¥æœŸèŒƒå›´ï¼Œè¿”å›ç©ºç»“æœ")
            return success_response(
                data={
                    "articles": [],
                    "pagination": {
                        "total": 0,
                        "page": page,
                        "page_size": page_size,
                        "total_pages": 0
                    }
                },
                request_id=request_id,
                source="database",
                cache_hit=True,
                cached_days=0,
                fetched_days=0
            )
        
        start_dt, _ = get_day_range(dates[0])
        _, end_dt = get_day_range(dates[-1])
        start_time_int = datetime_to_int(start_dt)
        end_time_int = datetime_to_int(end_dt)
        
        # æŸ¥è¯¢æ•°æ®
        result = repo.query_by_country_and_time(
            country_code, start_time_int, end_time_int, page, page_size
        )
        
        total_pages = (result["total"] + page_size - 1) // page_size if result["total"] > 0 else 0
        returned_count = len(result["data"])
        
        # è®¡ç®—è€—æ—¶
        duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        cache_status = "å…¨éƒ¨ç¼“å­˜" if fetched_days == 0 else f"æ–°è·å–{fetched_days}å¤©"
        
        logging.info(
            f"âœ… [{request_id}] è¯·æ±‚å®Œæˆ: æ€»å…±{result['total']}æ¡, æœ¬é¡µè¿”å›{returned_count}æ¡, "
            f"{cache_status}, è€—æ—¶{duration_ms}ms"
        )
        
        return success_response(
            data={
                "articles": result["data"],
                "pagination": {
                    "total": result["total"],
                    "page": result["page"],
                    "page_size": result["page_size"],
                    "total_pages": total_pages
                }
            },
            request_id=request_id,
            source="database",
            cache_hit=fetched_days == 0,
            cached_days=cached_days,
            fetched_days=fetched_days,
            duration_ms=duration_ms
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"âŒ [{request_id}] è·å–æ–‡ç« å¤±è´¥: {e}")
        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message=f"æœåŠ¡å™¨é”™è¯¯: {str(e)}",
            request_id=request_id
        )


@router.get("/stats")
async def get_stats():
    """
    è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    
    è¿”å›ï¼š
    - æ•°æ®åº“å¯ç”¨æ€§
    - æ–‡ç« æ€»æ•°
    - å­˜å‚¨ä½¿ç”¨é‡ä¼°ç®—ï¼ˆç›¸å¯¹äº Supabase å…è´¹ç‰ˆ 500MB é™åˆ¶ï¼‰
    - æŒ‰å›½å®¶åˆ†ç±»çš„æ–‡ç« æ•°é‡
    - ä½¿ç”¨ç‡è­¦å‘Šï¼ˆå¦‚æœæ¥è¿‘é™åˆ¶ï¼‰
    """
    request_id = str(uuid.uuid4())[:8]
    logging.info(f"ğŸ“Š [{request_id}] è¯·æ±‚ç»Ÿè®¡ä¿¡æ¯")
    
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            return success_response(
                data={
                    "database_available": False,
                    "message": "æ•°æ®åº“æœªé…ç½®"
                },
                request_id=request_id
            )
        
        # è·å–å®Œæ•´çš„å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        storage_stats = repo.get_storage_stats()
        
        logging.info(f"âœ… [{request_id}] ç»Ÿè®¡å®Œæˆ: {storage_stats['total_articles']}æ¡æ–‡ç« ")
        
        return success_response(
            data={
                "database_available": True,
                "total_articles": storage_stats["total_articles"],
                "storage": {
                    "estimated_size_mb": storage_stats["estimated_size_mb"],
                    "free_tier_limit_mb": storage_stats["free_tier_limit_mb"],
                    "usage_percent": storage_stats["usage_percent"],
                    "warning": storage_stats["warning"]
                },
                "articles_by_country": storage_stats["articles_by_country"]
            },
            request_id=request_id
        )
    except Exception as e:
        logging.error(f"âŒ [{request_id}] è·å–ç»Ÿè®¡å¤±è´¥: {e}")
        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message=str(e),
            request_id=request_id
        )
