"""
GDELT æ–°é—»æ•°æ®è·å–ä¸»ç¨‹åº
ä» BigQuery è·å– GDELT æ•°æ®å¹¶ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š
"""

import os
import pandas as pd
from datetime import datetime
from google.cloud import bigquery

# å¯¼å…¥æ•°æ®è§£ææ¨¡å—
from gdelt_parser import process_narrative

# å¯¼å…¥æŠ¥å‘Šè§£ææ¨¡å—
from parse_report import (
    parse_report,
    get_report_summary,
    search_reports,
    filter_by_criteria,
)

# å¯¼å…¥ LLM æ–°é—»ç”Ÿæˆæ¨¡å—
from llm_generator import generate_news_from_record, LLMNewsGenerator


# ================= é…ç½®åŒº =================
import pathlib
_SCRIPT_DIR = pathlib.Path(__file__).parent
KEY_PATH = str(_SCRIPT_DIR.parent.parent / 'gdelt_config' / 'my-gdelt-key.json')  # config åœ¨é¡¹ç›®æ ¹ç›®å½•
PROJECT_ID = 'gdelt-analysis-480906'

# ================= ä¼˜åŒ–ç‰ˆ SQL - ä½¿ç”¨åˆ†åŒºè¡¨å‡å°‘æ‰«ææˆæœ¬ =================
# å…³é”®ä¼˜åŒ–:
# 1. ä½¿ç”¨ gkg_partitioned åˆ†åŒºè¡¨è€Œé gkg
# 2. ä½¿ç”¨ _PARTITIONTIME ä¼ªåˆ—è¿›è¡Œåˆ†åŒºè£å‰ª (Partition Pruning)
# 3. è¿™æ · BigQuery åªæ‰«ææŒ‡å®šæ—¥æœŸåˆ†åŒºçš„æ•°æ®ï¼Œè€Œéå…¨è¡¨
# 4. é¢„è®¡æ‰«æé‡ä»æ•°ç™¾GBé™åˆ°å‡ GB
QUERY = """
SELECT
  GKGRECORDID,
  DATE,
  SourceCommonName,
  DocumentIdentifier AS SourceURL,
  CAST(SPLIT(V2Tone, ',')[OFFSET(0)] AS FLOAT64) AS AvgTone,
  V2Themes,
  V2Locations,
  V2Persons,
  V2Organizations,
  GCAM,
  Amounts,        
  Quotations,
  SocialImageEmbeds,
  SocialVideoEmbeds
FROM
  `gdelt-bq.gdeltv2.gkg_partitioned`
WHERE
  -- ä½¿ç”¨ _PARTITIONTIME è¿›è¡Œåˆ†åŒºè£å‰ªï¼Œåªæ‰«æä»Šå¤©çš„åˆ†åŒº
  _PARTITIONTIME >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)
  -- åœ¨åˆ†åŒºå†…å†æŒ‰ DATE å­—æ®µç²¾ç¡®è¿‡æ»¤åˆ°æœ€è¿‘2å°æ—¶
  AND DATE >= CAST(FORMAT_TIMESTAMP('%Y%m%d%H%M%S', TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 2 HOUR)) AS INT64)
  AND (V2Themes LIKE '%ENV_CLIMATECHANGE%' OR V2Themes LIKE '%CRISIS%')
  AND ABS(CAST(SPLIT(V2Tone, ',')[OFFSET(0)] AS FLOAT64)) > 3
  AND Quotations IS NOT NULL
ORDER BY
  ABS(AvgTone) DESC
LIMIT 50
"""


def fetch_gdelt_data() -> pd.DataFrame:
    """
    ä» BigQuery è·å– GDELT æ•°æ®
    
    Returns:
        åŒ…å« GDELT æ•°æ®çš„ DataFrameï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç©º DataFrame
    """
    if not os.path.exists(KEY_PATH):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°å¯†é’¥æ–‡ä»¶ {KEY_PATH}")
        return pd.DataFrame()

    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = KEY_PATH
    
    try:
        client = bigquery.Client(project=PROJECT_ID)
        print(f"[{datetime.now()}] å¼€å§‹æŸ¥è¯¢ BigQuery (ä½¿ç”¨åˆ†åŒºè¡¨ä¼˜åŒ–)...")
        
        # é¢„ä¼°æŸ¥è¯¢æˆæœ¬
        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
        dry_run_job = client.query(QUERY, job_config=job_config)
        bytes_processed = dry_run_job.total_bytes_processed
        gb_processed = bytes_processed / (1024**3)
        print(f"[é¢„ä¼°æ‰«æé‡] {gb_processed:.2f} GB")
        
        # æ‰§è¡Œå®é™…æŸ¥è¯¢
        query_job = client.query(QUERY) 
        results = query_job.result()

        df = results.to_dataframe()
        print(f"[{datetime.now()}] æŸ¥è¯¢å®Œæˆï¼Œè·å–åˆ° {len(df)} æ¡è®°å½•ã€‚")
        return df
        
    except Exception as e:
        print(f"BigQuery è¿æ¥æˆ–æŸ¥è¯¢é”™è¯¯: {e}")
        return pd.DataFrame()


def print_preview(result_df: pd.DataFrame, offset: int = 0, count: int = 5):
    """
    æ‰“å°æŠ¥å‘Šé¢„è§ˆ
    
    Args:
        result_df: ç»“æœ DataFrame
        offset: é¢„è§ˆè®°å½•èµ·å§‹ä½ç½®
        count: é¢„è§ˆè®°å½•æ•°é‡
    """

    for i, row in result_df.iloc[offset:offset+count].iterrows():
        print(f"\n--- è®°å½• {i+1} ---")
        print(f"ğŸ“Œ æ ‡é¢˜: {row['Title']}")
        print(f"ğŸ“° æ¥æº: {row['Source_Name']}")
        print(f"ğŸ“° æºURL: {row['Source_URL']}")
        print(f"ğŸ• æ—¶é—´: {row['Time']}")
        print(f"ğŸ“ åœ°ç‚¹: {row['Locations']}")
        print(f"ğŸ¢ æœºæ„: {row['Organizations']}")
        print(f"ğŸ‘¤ äººç‰©: {row['Key_Persons']}")
        print(f"ğŸ­ æƒ…æ„Ÿ: {row['Emotions']}")
        print(f"ğŸ“Š åŸºè°ƒ: {row['Tone']}")
        print(f"ğŸ·ï¸ ä¸»é¢˜: {row['Themes']}")
        print(f"ğŸ’¬ å¼•ç”¨:\n{row['Quotes'][:500]}...")
        print(f"ğŸ“ˆ æ•°æ®: {row['Data_Facts']}")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡: {row['Images'][:100]}...")
        print(f"ğŸ“° åŸæ–‡æ‘˜è¦:\n{row['Article_Summary'][:1000]}...")


def analyze_report(filename: str):
    """
    è§£æå¹¶åˆ†æç”Ÿæˆçš„æŠ¥å‘Š
    
    Args:
        filename: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    print("\n" + "="*60)
    print("ğŸ“Š æ­£åœ¨è§£æç”Ÿæˆçš„æŠ¥å‘Š...")
    print("="*60)
    
    try:
        # è§£ææŠ¥å‘Š
        report_result = parse_report(filename)
        
        # è·å–æ‘˜è¦ä¿¡æ¯
        summary = get_report_summary(filename)
        
        print(f"\nğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
        print(f"  - æ–‡ä»¶å: {summary['file_name']}")
        print(f"  - è®°å½•æ€»æ•°: {summary['record_count']}")
        print(f"  - å”¯ä¸€æ¥æºæ•°: {summary['source_count']}")
        
        print(f"\nğŸ­ æƒ…æ„Ÿåˆ†å¸ƒ:")
        for tone, count in summary['tone_stats'].items():
            if count > 0:
                percentage = summary.get('tone_percentages', {}).get(tone, 0)
                print(f"  - {tone}: {count} ({percentage}%)")
        
        print(f"\nğŸ“° ä¸»è¦æ¥æº:")
        for source in summary['top_sources'][:5]:
            print(f"  - {source}")
        
        # æœç´¢ç¤ºä¾‹ï¼šæŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®è¯çš„è®°å½•
        crisis_records = search_reports("crisis", filename)
        if crisis_records:
            print(f"\nğŸ” åŒ…å« 'crisis' å…³é”®è¯çš„è®°å½•: {len(crisis_records)} æ¡")
        
        # æŒ‰æƒ…æ„Ÿç­›é€‰ç¤ºä¾‹
        negative_records = filter_by_criteria(filename, tone="Negative")
        positive_records = filter_by_criteria(filename, tone="Positive")
        print(f"\nğŸ“ˆ æƒ…æ„Ÿç­›é€‰ç»“æœ:")
        print(f"  - è´Ÿé¢æŠ¥é“: {len(negative_records)} æ¡")
        print(f"  - æ­£é¢æŠ¥é“: {len(positive_records)} æ¡")
        
        print("\n" + "="*60)
        print("âœ… æŠ¥å‘Šè§£æå®Œæˆï¼")
        print("="*60)
        
    except Exception as parse_error:
        print(f"âš ï¸ æŠ¥å‘Šè§£ææ—¶å‡ºç°é”™è¯¯: {parse_error}")


def generate_news_with_llm(record: dict):
    """
    ä½¿ç”¨ LLM ç”Ÿæˆæ–°é—»æ–‡æœ¬
    
    Args:
        record: è§£æåçš„æ–°é—»è®°å½•å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ¤– æ­£åœ¨ä½¿ç”¨ LLM ç”Ÿæˆæ–°é—»æ–‡æœ¬...")
    print("="*60)
    
    print(f"\nğŸ“ è¾“å…¥æ•°æ®:")
    print(f"  - æ ‡é¢˜: {record.get('Title')}")
    print(f"  - æ¥æº: {record.get('Source_Name')}")
    print(f"  - åœ°ç‚¹: {record.get('Locations')}")
    print(f"  - ä¸»é¢˜: {record.get('Themes')}")
    
    try:
        news_text = generate_news_from_record(record)
        
        print(f"\nğŸ“° ç”Ÿæˆçš„æ–°é—»æ–‡æœ¬:")
        print("-" * 60)
        print(news_text)
        print("-" * 60)
        
        return news_text
    except Exception as e:
        error_msg = f"LLM ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(f"\nâš ï¸ {error_msg}")
        return error_msg


def main():
    """ä¸»å‡½æ•°"""
    # raw_df = fetch_gdelt_data()  # æ³¨é‡Šæ‰é¿å…æ¶ˆè€— BigQuery é¢åº¦
    
    # ä»ç°æœ‰ CSV æ–‡ä»¶è¯»å–æ•°æ®
    data_dir = _SCRIPT_DIR.parent.parent / '.data'
    raw_path = data_dir / "gdelt_raw_data.csv"
    if raw_path.exists():
        raw_df = pd.read_csv(raw_path)
        print(f"ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®: {raw_path}, å…± {len(raw_df)} æ¡è®°å½•")
    else:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {raw_path}")
        return
    
    if not raw_df.empty:
        try:
            # æ•°æ®ä¿å­˜ç›®å½•
            data_dir = _SCRIPT_DIR.parent.parent / '.data'
            data_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜åŸå§‹æ•°æ®
            raw_path = data_dir / "gdelt_raw_data.csv"
            raw_df.to_csv(raw_path, index=False, encoding='utf-8-sig')
            print(f"åŸå§‹æ•°æ®å·²ä¿å­˜è‡³: {raw_path}")
            
            # å¤„ç†æ•°æ®
            narratives = raw_df.apply(process_narrative, axis=1).tolist()
            result_df = pd.DataFrame(narratives)
            
            # æ‰“å°é¢„è§ˆ
            print_preview(result_df, offset=0, count=10)
            
            # ä¿å­˜ç»“æœ
            filename = f"gdelt_report_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
            report_path = data_dir / filename
            result_df.to_csv(report_path, index=False, encoding='utf-8-sig')
            print(f"\nâœ… å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
            
            # è§£ææŠ¥å‘Š
            analyze_report(str(report_path))
            
            # ================= LLM ç”Ÿæˆæ–°é—» =================
            if narratives:
                # å–ç¬¬0æ¡åˆ°ç¬¬10æ¡æ•°æ®è¿›è¡Œæ–°é—»ç”Ÿæˆ
                for i, record in enumerate(narratives[0:10], 1):
                    print(f"\n{'='*60}")
                    print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆç¬¬ {i}/10 æ¡æ–°é—»...")
                    print(f"{'='*60}")
                    generate_news_with_llm(record)
            else:
                print("\nâš ï¸ æ²¡æœ‰å¯ç”¨çš„æ•°æ®è®°å½•")
            
        except Exception as e:
            print(f"æ•°æ®è§£æé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("æœªè·å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ SQL ç­›é€‰æ¡ä»¶æˆ–ç½‘ç»œè¿æ¥ã€‚")


if __name__ == "__main__":
    main()