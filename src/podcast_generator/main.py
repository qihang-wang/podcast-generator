"""
GDELT æ–°é—»æ•°æ®è·å–ä¸»ç¨‹åº
ä» BigQuery è·å– GDELT æ•°æ®å¹¶ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š
"""

import os
import pandas as pd
from datetime import datetime

# å¯¼å…¥ GDELT æ•°æ®è·å–æ¨¡å—
from gdelt_fetcher import fetch_gdelt_data, load_local_data, save_data

# å¯¼å…¥æ•°æ®è§£ææ¨¡å—
from gdelt_parser import process_narrative

# å¯¼å…¥æ–°é—»åˆå¹¶æ¨¡å—
from news_merger import merge_related_news

# å¯¼å…¥æŠ¥å‘Šè§£ææ¨¡å—
from parse_report import (
    parse_report,
    get_report_summary,
    search_reports,
    filter_by_criteria,
)

# å¯¼å…¥ LLM æ–°é—»ç”Ÿæˆæ¨¡å—
from llm_generator import generate_news_from_record, LLMNewsGenerator


# ================= é…ç½®åŒº =================
import pathlib
_SCRIPT_DIR = pathlib.Path(__file__).parent
KEY_PATH = str(_SCRIPT_DIR.parent.parent / 'gdelt_config' / 'my-gdelt-key.json')
PROJECT_ID = 'gdelt-analysis-480906'

# GDELT æŸ¥è¯¢é…ç½®å·²ç§»è‡³ gdelt_fetcher.py


def print_preview(result_df: pd.DataFrame):
    """æ‰“å°å…¨é‡æŠ¥å‘Šé¢„è§ˆ"""
    total = len(result_df)
    print(f"\nğŸ“Š å…± {total} æ¡è®°å½•")
    
    for i, row in result_df.iterrows():
        print(f"\n--- è®°å½• {i+1}/{total} ---")
        print(f"ğŸ“Œ æ ‡é¢˜: {row['Title']}")
        print(f"ğŸ“° æ¥æº: {row['Source_Name']}")
        print(f"ğŸ“° æºURL: {row['Source_URL']}")
        print(f"ğŸ• æ—¶é—´: {row['Time']}")
        print(f"ğŸ“ åœ°ç‚¹: {row['Locations']}")
        print(f"ğŸ¢ æœºæ„: {row['Organizations']}")
        print(f"ğŸ‘¤ äººç‰©: {row['Key_Persons']}")
        print(f"ğŸ­ æƒ…æ„Ÿ: {row['Emotions']}")
        print(f"ğŸ“Š åŸºè°ƒ: {row['Tone']}")
        print(f"ğŸ·ï¸ ä¸»é¢˜: {row['Themes']}")
        quotes = row['Quotes'][:500] if len(str(row['Quotes'])) > 500 else row['Quotes']
        print(f"ğŸ’¬ å¼•ç”¨:\n{quotes}...")
        print(f"ğŸ“ˆ æ•°æ®: {row['Data_Facts']}")
        images = row['Images'][:100] if len(str(row['Images'])) > 100 else row['Images']
        print(f"ğŸ–¼ï¸ å›¾ç‰‡: {images}...")
        summary = row['Article_Summary'][:1000] if len(str(row['Article_Summary'])) > 1000 else row['Article_Summary']
        print(f"ğŸ“° åŸæ–‡æ‘˜è¦:\n{summary}...")


def analyze_report(filename: str):
    """è§£æå¹¶åˆ†æç”Ÿæˆçš„æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š æ­£åœ¨è§£æç”Ÿæˆçš„æŠ¥å‘Š...")
    print("="*60)
    
    try:
        report_result = parse_report(filename)
        summary = get_report_summary(filename)
        
        print(f"\nğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
        print(f"  - æ–‡ä»¶å: {summary['file_name']}")
        print(f"  - è®°å½•æ€»æ•°: {summary['record_count']}")
        print(f"  - å”¯ä¸€æ¥æºæ•°: {summary['source_count']}")
        
        print(f"\nğŸ­ æƒ…æ„Ÿåˆ†å¸ƒ:")
        for tone, count in summary['tone_stats'].items():
            if count > 0:
                percentage = summary.get('tone_percentages', {}).get(tone, 0)
                print(f"  - {tone}: {count} ({percentage}%)")
        
        print(f"\nğŸ“° ä¸»è¦æ¥æº:")
        for source in summary['top_sources'][:5]:
            print(f"  - {source}")
        
        crisis_records = search_reports("crisis", filename)
        if crisis_records:
            print(f"\nğŸ” åŒ…å« 'crisis' å…³é”®è¯çš„è®°å½•: {len(crisis_records)} æ¡")
        
        negative_records = filter_by_criteria(filename, tone="Negative")
        positive_records = filter_by_criteria(filename, tone="Positive")
        print(f"\nğŸ“ˆ æƒ…æ„Ÿç­›é€‰ç»“æœ:")
        print(f"  - è´Ÿé¢æŠ¥é“: {len(negative_records)} æ¡")
        print(f"  - æ­£é¢æŠ¥é“: {len(positive_records)} æ¡")
        
        print("\n" + "="*60)
        print("âœ… æŠ¥å‘Šè§£æå®Œæˆï¼")
        print("="*60)
        
    except Exception as parse_error:
        print(f"âš ï¸ æŠ¥å‘Šè§£ææ—¶å‡ºç°é”™è¯¯: {parse_error}")


def generate_bilingual_news(record: dict) -> tuple:
    """
    ç”ŸæˆåŒè¯­æ–°é—»ï¼ˆè‹±æ–‡ + ä¸­æ–‡ï¼‰
    
    Args:
        record: è§£æåçš„æ–°é—»è®°å½•å­—å…¸
    
    Returns:
        (è‹±æ–‡æ–°é—», ä¸­æ–‡æ–°é—») å…ƒç»„
    """
    print(f"\nğŸ“ è¾“å…¥æ•°æ®:")
    print(f"  - æ ‡é¢˜: {record.get('Title')}")
    print(f"  - æ¥æº: {record.get('Source_Name')}")
    locations = record.get('Locations', '')
    print(f"  - åœ°ç‚¹: {locations[:80]}..." if len(locations) > 80 else f"  - åœ°ç‚¹: {locations}")
    
    # ç”Ÿæˆè‹±æ–‡æ–°é—»
    print(f"\nğŸ”¤ ç”Ÿæˆè‹±æ–‡æ–°é—»...")
    try:
        english_news = generate_news_from_record(record, language="en")
        print(f"\nğŸ“° English News:")
        print("-" * 60)
        print(english_news)
        print("-" * 60)
    except Exception as e:
        english_news = f"[Error] {str(e)}"
        print(f"  âš ï¸ è‹±æ–‡ç”Ÿæˆå¤±è´¥: {e}")
    
    # ç”Ÿæˆä¸­æ–‡æ–°é—»
    print(f"\nğŸ”¤ ç”Ÿæˆä¸­æ–‡æ–°é—»...")
    try:
        chinese_news = generate_news_from_record(record, language="zh")
        print(f"\nğŸ“° ä¸­æ–‡æ–°é—»:")
        print("-" * 60)
        print(chinese_news)
        print("-" * 60)
    except Exception as e:
        chinese_news = f"[Error] {str(e)}"
        print(f"  âš ï¸ ä¸­æ–‡ç”Ÿæˆå¤±è´¥: {e}")
    
    return english_news, chinese_news


def main():
    """ä¸»å‡½æ•°"""
    # æ–¹å¼1: ä» BigQuery è·å–æ•°æ®ï¼ˆæ¶ˆè€—é¢åº¦ï¼Œè°¨æ…ä½¿ç”¨ï¼‰
    # raw_df = fetch_gdelt_data(key_path=KEY_PATH, project_id=PROJECT_ID)
    
    # æ–¹å¼2: ä»æœ¬åœ°æ–‡ä»¶è¯»å–æ•°æ®
    data_dir = _SCRIPT_DIR.parent.parent / '.data'
    raw_path = data_dir / "gdelt_raw_data.csv"
    raw_df = load_local_data(str(raw_path))
    
    if raw_df.empty:
        print("é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶æˆ–æ•°æ®ä¸ºç©º")
        return
    
    try:
        # æ•°æ®ä¿å­˜ç›®å½•
        data_dir = _SCRIPT_DIR.parent.parent / '.data'
        data_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜åŸå§‹æ•°æ®
        raw_path = data_dir / "gdelt_raw_data.csv"
        save_data(raw_df, str(raw_path))
        
        # å¤„ç†æ•°æ®
        narratives = raw_df.apply(process_narrative, axis=1).tolist()
        
        # åˆå¹¶ç›¸å…³/é‡å¤çš„æ–°é—»è®°å½•
        merged_narratives = merge_related_news(narratives, similarity_threshold=0.6)
        
        # ä½¿ç”¨åˆå¹¶åçš„æ•°æ®åˆ›å»º DataFrame
        result_df = pd.DataFrame(merged_narratives)
        
        # æ‰“å°å…¨é‡é¢„è§ˆ
        print_preview(result_df)
        
        # ä¿å­˜ç»“æœ
        filename = f"gdelt_report_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
        report_path = data_dir / filename
        save_data(result_df, str(report_path))
        
        # è§£ææŠ¥å‘Š
        analyze_report(str(report_path))
        
        # ================= LLM ç”ŸæˆåŒè¯­æ–°é—» =================
        print(f"\n{'='*60}")
        print(f"ğŸ¤– å¼€å§‹ç”ŸæˆåŒè¯­æ–°é—»ï¼ˆè‹±æ–‡ + ä¸­æ–‡ï¼‰")
        print(f"ğŸ“Š å…± {len(merged_narratives)} æ¡æ–°é—»å¾…ç”Ÿæˆ")
        print(f"{'='*60}")
        
        if merged_narratives:
            all_news = []
            for i, record in enumerate(merged_narratives, 1):
                print(f"\n{'='*60}")
                print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆç¬¬ {i}/{len(merged_narratives)} æ¡åŒè¯­æ–°é—»...")
                print(f"{'='*60}")
                en_news, zh_news = generate_bilingual_news(record)
                all_news.append({
                    'title': record.get('Title'),
                    'source': record.get('Source_Name'),
                    'english': en_news,
                    'chinese': zh_news
                })
            
            # æ±‡æ€»
            print(f"\n{'='*60}")
            print(f"âœ… åŒè¯­æ–°é—»ç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ“Š å…±ç”Ÿæˆ {len(all_news)} æ¡åŒè¯­æ–°é—»")
            print(f"{'='*60}")
        else:
            print("\nâš ï¸ æ²¡æœ‰å¯ç”¨çš„æ•°æ®è®°å½•")
        
    except Exception as e:
        print(f"æ•°æ®è§£æé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()