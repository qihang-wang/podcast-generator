"""
æ–‡ç« æ•°æ® API è·¯ç”±
æ”¯æŒä» CSV æˆ– Supabase è·å–æ•°æ®
"""

from fastapi import APIRouter, Query, HTTPException
from typing import Optional
from datetime import datetime, timedelta
import logging

router = APIRouter(prefix="/api/articles", tags=["æ–‡ç« æ•°æ®"])


def _get_time_range(days: int):
    """è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆè¿”å› YYYYMMDDHHMMSS æ ¼å¼çš„æ•´æ•°ï¼‰"""
    now = datetime.now()
    end_time = int(now.strftime("%Y%m%d%H%M%S"))
    start_time = int((now - timedelta(days=days)).strftime("%Y%m%d%H%M%S"))
    return start_time, end_time


@router.get("/")
async def get_articles(
    country_code: str = Query("CH", description="å›½å®¶ä»£ç  (FIPS 10-4)"),
    days: int = Query(1, ge=1, le=7, description="è·å–æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆ1-7å¤©ï¼‰"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    fetch_content: bool = Query(False, description="æ˜¯å¦è·å–æ–‡ç« å…¨æ–‡ï¼ˆä»…CSVæ¨¡å¼ï¼‰"),
    use_database: bool = Query(True, description="æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“")
):
    """
    è·å–æŒ‡å®šå›½å®¶çš„æ–‡ç« æ•°æ®
    
    æ•°æ®æºä¼˜å…ˆçº§ï¼š
    1. å¦‚æœ use_database=true ä¸”æ•°æ®åº“æœ‰ç¼“å­˜ â†’ ç›´æ¥è¿”å›
    2. å¦åˆ™ä» CSV åŠ è½½ï¼ˆå¦‚éœ€è¦ä¼šå…ˆä» BigQuery è·å–ï¼‰
    
    å‚æ•°ï¼š
    - **country_code**: å›½å®¶ä»£ç ï¼Œå¦‚ "CH"=ä¸­å›½, "US"=ç¾å›½
    - **days**: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆ1-7å¤©ï¼‰
    - **page**: é¡µç 
    - **page_size**: æ¯é¡µæ•°é‡
    - **use_database**: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ï¼ˆé»˜è®¤ trueï¼‰
    """
    try:
        # å°è¯•ä»æ•°æ®åº“è·å–
        if use_database:
            result = await _get_from_database(country_code, days, page, page_size)
            if result["has_data"]:
                return {
                    "success": True,
                    "source": "supabase",
                    "cache_hit": result["cache_hit"],
                    "data": result["data"],
                    "total": result["total"],
                    "page": result["page"],
                    "page_size": result["page_size"],
                    "total_pages": (result["total"] + page_size - 1) // page_size if result["total"] > 0 else 0
                }
        
        # å›é€€åˆ° CSV
        result = await _get_from_csv(country_code, fetch_content)
        
        # æ‰‹åŠ¨åˆ†é¡µ
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        paginated_data = result["data"][start_idx:end_idx]
        
        return {
            "success": True,
            "source": "csv",
            "cache_hit": False,
            "data": paginated_data,
            "total": result["total"],
            "page": page,
            "page_size": page_size,
            "total_pages": (result["total"] + page_size - 1) // page_size if result["total"] > 0 else 0
        }
    
    except FileNotFoundError as e:
        raise HTTPException(
            status_code=404,
            detail=f"æœªæ‰¾åˆ°å›½å®¶ä»£ç  {country_code} çš„æ•°æ®æ–‡ä»¶"
        )
    except Exception as e:
        logging.error(f"è·å–æ–‡ç« å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
        )


async def _get_from_database(
    country_code: str, 
    days: int, 
    page: int, 
    page_size: int
) -> dict:
    """ä» Supabase æ•°æ®åº“è·å–æ•°æ®"""
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            return {"has_data": False}
        
        start_time, end_time = _get_time_range(days)
        
        # æ£€æŸ¥ç¼“å­˜è¦†ç›–æƒ…å†µ
        coverage = repo.check_cache_coverage(country_code, start_time, end_time)
        
        if not coverage["has_data"]:
            # æ•°æ®åº“æ— æ•°æ®ï¼Œéœ€è¦è·å–
            await _fetch_missing_data(country_code, days)
        
        # æŸ¥è¯¢æ•°æ®
        result = repo.query_by_country_and_time(
            country_code, start_time, end_time, page, page_size
        )
        
        return {
            "has_data": len(result["data"]) > 0,
            "cache_hit": coverage["covered"],
            "data": result["data"],
            "total": result["total"],
            "page": result["page"],
            "page_size": result["page_size"]
        }
    
    except ImportError:
        return {"has_data": False}
    except Exception as e:
        logging.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        return {"has_data": False}


async def _fetch_missing_data(country_code: str, days: int):
    """è·å–ç¼ºå¤±çš„æ•°æ®ï¼ˆä» BigQueryï¼‰"""
    try:
        from podcast_generator.gdelt.data_fetcher import fetch_gkg_data
        
        hours_back = days * 24
        logging.info(f"ğŸ“¥ ä» BigQuery è·å– {country_code} æœ€è¿‘ {hours_back} å°æ—¶æ•°æ®...")
        
        # è¿™ä¼šä¿å­˜åˆ° CSV å¹¶åŒæ­¥åˆ°æ•°æ®åº“
        fetch_gkg_data(
            country_code=country_code,
            hours_back=hours_back,
            limit=100
        )
    except Exception as e:
        logging.error(f"è·å–æ•°æ®å¤±è´¥: {e}")


async def _get_from_csv(country_code: str, fetch_content: bool) -> dict:
    """ä» CSV æ–‡ä»¶è·å–æ•°æ®"""
    from podcast_generator.gdelt.data_loader import load_gdelt_data
    from podcast_generator.gdelt.gdelt_parse import parse_gdelt_article
    
    # åŠ è½½æ•°æ®
    gkg_models, event_models = load_gdelt_data(country_code=country_code)
    
    # å»ºç«‹ Event æ˜ å°„
    events_dict = {e.global_event_id: e for e in event_models}
    
    # è§£ææ¯ç¯‡æ–‡ç« 
    articles = []
    for gkg in gkg_models:
        event = events_dict.get(gkg.event_id)
        params = parse_gdelt_article(gkg, event, fetch_content=fetch_content)
        articles.append(params)
    
    return {
        "data": articles,
        "total": len(articles)
    }


@router.get("/stats")
async def get_stats():
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            return {
                "success": True,
                "database_available": False,
                "message": "æ•°æ®åº“æœªé…ç½®"
            }
        
        total = repo.get_article_count()
        
        return {
            "success": True,
            "database_available": True,
            "total_articles": total
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


@router.post("/cleanup")
async def cleanup_old_articles(days: int = Query(7, description="ä¿ç•™æœ€è¿‘Nå¤©çš„æ•°æ®")):
    """æ¸…ç†è¿‡æœŸæ•°æ®"""
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            raise HTTPException(status_code=503, detail="æ•°æ®åº“æœªé…ç½®")
        
        deleted = repo.cleanup_old_articles(days=days)
        
        return {
            "success": True,
            "deleted_count": deleted,
            "message": f"å·²æ¸…ç† {deleted} æ¡è¶…è¿‡ {days} å¤©çš„æ•°æ®"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
