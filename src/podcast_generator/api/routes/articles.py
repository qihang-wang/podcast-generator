"""
æ–‡ç« æ•°æ® API è·¯ç”±
ç®€åŒ–ç‰ˆï¼šåªæ”¯æŒ days=0ï¼ˆä»Šå¤©ï¼‰å’Œ days=1ï¼ˆæ˜¨å¤©ï¼‰
æ¯å¤©çš„æ•°æ®åªä¼šä» BigQuery è·å–ä¸€æ¬¡ï¼Œä¹‹åä½¿ç”¨ Supabase ç¼“å­˜
"""

from fastapi import APIRouter, Query, HTTPException
from datetime import datetime, timedelta
import logging
import uuid

from .articles_helpers import (
    get_day_range,
    datetime_to_int,
    check_day_cached,
    fetch_day_data_with_lock,
)
from podcast_generator.api.response import success_response, error_response, ErrorCode

router = APIRouter(prefix="/api/articles", tags=["æ–‡ç« æ•°æ®"])


@router.get("/")
async def get_articles(
    country_code: str = Query("CH", description="å›½å®¶ä»£ç  (FIPS 10-4)"),
    days: int = Query(0, description="0=ä»Šå¤©ï¼ˆé»˜è®¤ï¼‰, 1=æ˜¨å¤©"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡")
):
    """
    è·å–æŒ‡å®šå›½å®¶çš„æ–‡ç« æ•°æ®
    
    ç®€åŒ–ç­–ç•¥ï¼š
    - **days=0**: è·å–ä»Šå¤©çš„æ•°æ®ï¼ˆé»˜è®¤ï¼‰
    - **days=1**: è·å–æ˜¨å¤©çš„æ•°æ®
    
    æ¯å¤©çš„æ•°æ®åªä¼šä» BigQuery è·å–ä¸€æ¬¡ï¼Œä¹‹åä½¿ç”¨ Supabase ç¼“å­˜ã€‚
    
    å‚æ•°ï¼š
    - **country_code**: å›½å®¶ä»£ç ï¼Œå¦‚ "CH"=ä¸­å›½, "US"=ç¾å›½
    - **days**: 0=ä»Šå¤©ï¼ˆé»˜è®¤ï¼‰, 1=æ˜¨å¤©
    - **page**: é¡µç ï¼ˆé»˜è®¤1ï¼‰
    - **page_size**: æ¯é¡µæ•°é‡ï¼ˆé»˜è®¤20ï¼‰
    """
    request_id = str(uuid.uuid4())[:8]
    request_start_time = datetime.now()
    
    logging.info(f"\n\n")
    logging.info("=" * 60)
    logging.info(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚ [{request_id}]: country={country_code}, days={days}, page={page}, page_size={page_size}")
    
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            logging.error(f"âŒ [{request_id}] æ•°æ®åº“ä¸å¯ç”¨")
            return error_response(
                code=ErrorCode.DATABASE_UNAVAILABLE,
                message="æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ Supabase é…ç½®",
                request_id=request_id
            )
        
        # éªŒè¯ days å‚æ•°
        if days not in (0, 1):
            logging.warning(f"âš ï¸ [{request_id}] æ— æ•ˆçš„ days å‚æ•°: {days}")
            return error_response(
                code=ErrorCode.INVALID_PARAMETER,
                message=f"days å‚æ•°åªæ”¯æŒ 0ï¼ˆä»Šå¤©ï¼‰æˆ– 1ï¼ˆæ˜¨å¤©ï¼‰ï¼Œæ”¶åˆ°: {days}",
                request_id=request_id
            )
        
        # è®¡ç®—ç›®æ ‡æ—¥æœŸ
        today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        if days == 0:
            target_date = today
            date_label = "ä»Šå¤©"
        else:  # days == 1
            target_date = today - timedelta(days=1)
            date_label = "æ˜¨å¤©"
        
        target_date_str = target_date.strftime("%Y-%m-%d")
        logging.info(f"ğŸ“… [{request_id}] æŸ¥è¯¢{date_label}çš„æ•°æ®: {target_date_str}")
        
        # æ£€æŸ¥ç¼“å­˜å¹¶æŒ‰éœ€è·å–ï¼ˆæ¯å¤©åªè·å–ä¸€æ¬¡ï¼‰
        fetched = False
        if not check_day_cached(repo, country_code, target_date):
            logging.info(f"â—‹ [{request_id}] {target_date_str} æœªç¼“å­˜ï¼Œä» BigQuery è·å–...")
            fetched = await fetch_day_data_with_lock(repo, country_code, target_date)
        
        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆç›®æ ‡æ—¥æœŸçš„ 00:00:00 ~ 23:59:59ï¼‰
        day_start, day_end = get_day_range(target_date)
        
        # å¯¹äºä»Šå¤©ï¼Œç»“æŸæ—¶é—´æ˜¯å½“å‰æ—¶åˆ»
        if days == 0:
            day_end = datetime.now()
        
        start_int = datetime_to_int(day_start)
        end_int = datetime_to_int(day_end)
        
        # æŸ¥è¯¢æ•°æ®
        result = repo.query_by_country_and_time(
            country_code, start_int, end_int, page, page_size
        )
        
        total_pages = (result["total"] + page_size - 1) // page_size if result["total"] > 0 else 0
        returned_count = len(result["data"])
        duration_ms = int((datetime.now() - request_start_time).total_seconds() * 1000)
        
        cache_status = "ä» BigQuery è·å–" if fetched else "ä½¿ç”¨ç¼“å­˜"
        logging.info(
            f"âœ… [{request_id}] è¯·æ±‚å®Œæˆ: {date_label} ({target_date_str}), "
            f"æ€»å…±{result['total']}æ¡, æœ¬é¡µ{returned_count}æ¡, {cache_status}, è€—æ—¶{duration_ms}ms"
        )
        
        return success_response(
            data={
                "articles": result["data"],
                "date": target_date_str,
                "pagination": {
                    "total": result["total"],
                    "page": result["page"],
                    "page_size": result["page_size"],
                    "total_pages": total_pages
                }
            },
            request_id=request_id,
            source="database",
            is_today=(days == 0),
            cache_hit=not fetched,
            duration_ms=duration_ms
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"âŒ [{request_id}] è·å–æ–‡ç« å¤±è´¥: {e}")
        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message=f"æœåŠ¡å™¨é”™è¯¯: {str(e)}",
            request_id=request_id
        )


@router.get("/stats")
async def get_stats():
    """
    è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    
    è¿”å›ï¼š
    - æ•°æ®åº“å¯ç”¨æ€§
    - æ–‡ç« æ€»æ•°
    - å­˜å‚¨ä½¿ç”¨é‡ä¼°ç®—ï¼ˆç›¸å¯¹äº Supabase å…è´¹ç‰ˆ 500MB é™åˆ¶ï¼‰
    - æŒ‰å›½å®¶åˆ†ç±»çš„æ–‡ç« æ•°é‡
    - ä½¿ç”¨ç‡è­¦å‘Šï¼ˆå¦‚æœæ¥è¿‘é™åˆ¶ï¼‰
    """
    request_id = str(uuid.uuid4())[:8]
    logging.info(f"ğŸ“Š [{request_id}] è¯·æ±‚ç»Ÿè®¡ä¿¡æ¯")
    
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            return success_response(
                data={
                    "database_available": False,
                    "message": "æ•°æ®åº“æœªé…ç½®"
                },
                request_id=request_id
            )
        
        # è·å–å®Œæ•´çš„å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        storage_stats = repo.get_storage_stats()
        
        logging.info(f"âœ… [{request_id}] ç»Ÿè®¡å®Œæˆ: {storage_stats['total_articles']}æ¡æ–‡ç« ")
        
        return success_response(
            data={
                "database_available": True,
                "total_articles": storage_stats["total_articles"],
                "storage": {
                    "estimated_size_mb": storage_stats["estimated_size_mb"],
                    "free_tier_limit_mb": storage_stats["free_tier_limit_mb"],
                    "usage_percent": storage_stats["usage_percent"],
                    "warning": storage_stats["warning"]
                },
                "articles_by_country": storage_stats["articles_by_country"]
            },
            request_id=request_id
        )
    except Exception as e:
        logging.error(f"âŒ [{request_id}] è·å–ç»Ÿè®¡å¤±è´¥: {e}")
        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message=str(e),
            request_id=request_id
        )
