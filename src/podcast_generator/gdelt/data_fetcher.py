"""
GDELT æ•°æ®è·å–æ¨¡å—
ä» BigQuery è·å– Event -> Mentions -> GKG å®Œæ•´æ•°æ®æµ

å…¬å¼€æ–¹æ³•ï¼š
    - fetch_gdelt_data: è·å– GDELT æ•°æ®çš„å”¯ä¸€å…¥å£
    - fetch_gkg_data: ç›´æ¥è·å– GKG æ•°æ®
"""

import os
import logging
import pandas as pd
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Any, Optional

from .gdelt_service import GDELTQueryService
from .gdelt_mentions import select_best_mentions_per_event


# ========== ç§æœ‰å¸¸é‡ ==========
_GDELT_DATA_DIR = os.path.join(os.path.dirname(__file__), "gdelt_data")


def fetch_gdelt_data(location_name: str = None, country_code: str = None,
                     hours_back: int = None, date: str = None,
                     event_limit: int = 100, min_confidence: int = 80, max_sentence_id: int = 1):
    """è·å– GDELT å®Œæ•´æ•°æ®ï¼ˆEvent -> Mentions -> GKGï¼‰å¹¶ä¿å­˜åˆ° CSV"""
    logging.info("\n" + "=" * 80)
    logging.info("ğŸš€ å¼€å§‹ GDELT æ•°æ®è·å–")
    logging.info("=" * 80)
    
    service = GDELTQueryService()
    
    # Step 1: è·å–äº‹ä»¶
    logging.info(f"\nğŸ“ æ­¥éª¤ 1/3: æŸ¥è¯¢ Event è¡¨")
    if date:
        logging.info(f"   å‚æ•°: location={location_name or 'ä¸é™'}, country={country_code or 'ä¸é™'}, æ—¥æœŸ={date}")
    else:
        logging.info(f"   å‚æ•°: location={location_name or 'ä¸é™'}, country={country_code or 'ä¸é™'}, hours={hours_back or 24}h")
    
    events = service.query_events_by_location(
        location_name=location_name, country_code=country_code,
        hours_back=hours_back, date=date, limit=event_limit, print_progress=True
    )
    
    if not events:
        logging.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº‹ä»¶")
        return
    
    logging.info(f"âœ“ æ‰¾åˆ° {len(events)} ä¸ªäº‹ä»¶")
    
    # Step 2: è·å– Mentions
    logging.info(f"\nğŸ“° æ­¥éª¤ 2/3: æŸ¥è¯¢ Mentions è¡¨")
    logging.info(f"   å‚æ•°: Confidence>={min_confidence}%, SentenceID<={max_sentence_id}")
    
    event_ids = [e.global_event_id for e in events]
    all_mentions = service.query_mentions_by_event_ids(
        event_ids=event_ids,
        min_confidence=min_confidence,
        sentence_id=max_sentence_id,
        print_progress=True
    )
    
    if not all_mentions:
        logging.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æŠ¥é“")
        return
    
    # æ‰“å°äº‹ä»¶æ±‡æ€»
    _print_event_summary(events, all_mentions)
    
    # ç­›é€‰æœ€ä½³æŠ¥é“
    all_mentions = select_best_mentions_per_event(all_mentions)
    
    # ç­›é€‰å‡ºä¸ mentions ç›¸å…³çš„äº‹ä»¶
    related_event_ids = set(m.global_event_id for m in all_mentions)
    related_events = [e for e in events if e.global_event_id in related_event_ids]
    logging.info(f"âœ“ ç­›é€‰å‡º {len(related_events)} ä¸ªç›¸å…³äº‹ä»¶")
    
    # Step 3: è·å– GKG æ•°æ®
    logging.info(f"\nğŸ” æ­¥éª¤ 3/3: æŸ¥è¯¢ GKG è¡¨")
    
    mention_urls = [m.mention_identifier for m in all_mentions if m.mention_identifier]
    if not mention_urls:
        logging.warning("âš ï¸ æ— æœ‰æ•ˆ URL")
        return
    
    gkg_df = service.query_gkg_raw(mention_urls, print_progress=True)
    
    if gkg_df.empty:
        logging.warning("âš ï¸ æœªè·å–åˆ° GKG æ•°æ®")
        return
    
    # å»ºç«‹ URL -> EventID æ˜ å°„ï¼Œæ·»åŠ åˆ° GKG DataFrame
    url_to_event = {m.mention_identifier: m.global_event_id for m in all_mentions}
    gkg_df['event_id'] = gkg_df['DocumentIdentifier'].map(url_to_event)
    
    logging.info(f"âœ“ è·å–åˆ° {len(gkg_df)} æ¡ GKG æ•°æ®ï¼Œå·²å…³è” event_id")
    
    # ä¿å­˜åˆ° CSV
    _save_gkg_to_csv(gkg_df, country_code)
    _save_events_to_csv(related_events, country_code)
    
    # åŒæ­¥åˆ°æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    _sync_to_supabase(gkg_df, country_code)

    
    # å®Œæˆ
    logging.info("\n" + "=" * 80)
    logging.info(f"âœ… å®Œæˆï¼{len(related_events)} ä¸ªäº‹ä»¶ï¼Œ{len(gkg_df)} ç¯‡æ–‡ç« ")
    logging.info("=" * 80 + "\n")


def fetch_gkg_data(country_code: str, hours_back: int = None, date: str = None,
                   themes: list = None, allowed_languages: list = None,
                   min_word_count: int = 200, limit: int = 20):
    """ç›´æ¥è·å– GKG æ•°æ®å¹¶ä¿å­˜åˆ° CSV"""
    logging.info("\n" + "=" * 80)
    logging.info("ğŸš€ å¼€å§‹ GKG æ•°æ®ç›´æ¥è·å–")
    logging.info("=" * 80)
    
    if date:
        logging.info(f"\nğŸ“ å‚æ•°: country={country_code}, æ—¥æœŸ={date}, limit={limit}")
    else:
        logging.info(f"\nğŸ“ å‚æ•°: country={country_code}, hours={hours_back or 24}h, limit={limit}")
    
    if themes:
        logging.info(f"   ä¸»é¢˜è¿‡æ»¤: {themes}")
    if allowed_languages:
        logging.info(f"   è¯­è¨€è¿‡æ»¤: {allowed_languages}")
    
    service = GDELTQueryService()
    
    logging.info(f"\nğŸ” æŸ¥è¯¢ GKG è¡¨...")
    gkg_df = service.query_gkg_by_country(
        country_code=country_code, hours_back=hours_back, date=date,
        themes=themes, allowed_languages=allowed_languages,
        min_word_count=min_word_count, limit=limit, print_progress=True
    )
    
    if gkg_df.empty:
        logging.warning("âš ï¸ æœªè·å–åˆ° GKG æ•°æ®")
        return
    
    logging.info(f"âœ“ è·å–åˆ° {len(gkg_df)} æ¡ GKG æ•°æ®")
    
    # å…ˆå»é‡ï¼ˆç¡®ä¿ä¿å­˜å’ŒåŒæ­¥ä½¿ç”¨ç›¸åŒçš„å»é‡åæ•°æ®ï¼‰
    gkg_df = _deduplicate_by_url(gkg_df)
    
    # ä¿å­˜åˆ° CSV
    _save_gkg_to_csv(gkg_df, country_code, skip_dedup=True)
    
    # åŒæ­¥åˆ°æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    _sync_to_supabase(gkg_df, country_code)
    
    # å®Œæˆ
    logging.info("\n" + "=" * 80)
    logging.info(f"âœ… å®Œæˆï¼{len(gkg_df)} ç¯‡æ–‡ç« ")
    logging.info("=" * 80 + "\n")




# ========== æ•°æ®åº“åŒæ­¥ ==========

def _sync_to_supabase(gkg_df: pd.DataFrame, country_code: str):
    """
    å°† GKG æ•°æ®åŒæ­¥åˆ° Supabaseï¼ˆæŒ‰æ—¶é—´æ’åºå­˜å‚¨ï¼‰
    
    ä»…åœ¨ ENABLE_DATABASE_SYNC=true æ—¶æ‰§è¡Œ
    """
    try:
        from podcast_generator.database import ArticleRepository
        from podcast_generator.gdelt.gdelt_parse import parse_gdelt_article
        from .gdelt_gkg import _row_to_gkg_model
        
        repo = ArticleRepository()
        
        if not repo.is_sync_enabled():
            logging.debug("æ•°æ®åº“åŒæ­¥æœªå¯ç”¨ï¼Œè·³è¿‡")
            return
        
        logging.info("\nğŸ“¤ åŒæ­¥æ•°æ®åˆ° Supabase...")
        
        records = []
        for _, row in gkg_df.iterrows():
            gkg = _row_to_gkg_model(row)
            params = parse_gdelt_article(gkg, event=None, fetch_content=False)
            
            # Lite Mode: ç²¾ç®€å­—æ®µï¼Œç§»é™¤å·²ä¸å†ä» BigQuery è·å–çš„å­—æ®µ
            record = {
                "country_code": country_code.upper() if country_code else "UNKNOWN",
                "gkg_record_id": gkg.gkg_record_id,  # åŒ…å«æ—¶é—´æˆ³ï¼Œç”¨äºæ’åº
                "date_added": gkg.date,  # GDELT æ‰¹æ¬¡æ—¶é—´æˆ³ï¼ˆç”¨äºæŸ¥è¯¢è¿‡æ»¤ï¼Œä¸ BigQuery ä¸€è‡´ï¼‰
                "source": params.get("source"),
                "url": params.get("url"),
                "persons": params.get("persons", []),
                "organizations": params.get("organizations", []),
                "themes": params.get("themes", []),
                "locations": params.get("locations", []),
                "quotations": params.get("quotations", []),
                "amounts": params.get("amounts", []),
                "tone": params.get("tone"),
                "emotion": params.get("emotion"),
                "emotion_instruction": params.get("emotion_instruction"),
                "event": params.get("event"),
            }
            records.append(record)
        
        # æ‰¹é‡æ’å…¥
        count = repo.bulk_upsert(records)
        logging.info(f"âœ… å·²åŒæ­¥ {count} æ¡æ•°æ®åˆ° Supabase")
        
    except ImportError as e:
        logging.debug(f"æ•°æ®åº“æ¨¡å—æœªå®‰è£…: {e}")
    except Exception as e:
        logging.error(f"âŒ Supabase åŒæ­¥å¤±è´¥: {e}")


# ========== ç§æœ‰æ–¹æ³• ==========

def _deduplicate_by_url(gkg_df: pd.DataFrame) -> pd.DataFrame:
    """
    åŸºäº URL å»é‡ï¼Œç§»é™¤é‡å¤æ–‡ç« 
    
    åŒä¸€ URL ä¸åº”å‡ºç°å¤šæ¬¡ã€‚
    """
    if 'DocumentIdentifier' not in gkg_df.columns:
        return gkg_df
    
    # è®°å½•åŸå§‹æ•°é‡
    original_count = len(gkg_df)
    
    # æ‰¾å‡ºé‡å¤çš„è®°å½•ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼Œæ ‡è®°å…¶ä½™ä¸ºé‡å¤ï¼‰
    duplicates = gkg_df[gkg_df.duplicated(subset=['DocumentIdentifier'], keep='first')]
    
    # æ‰“å°è¢«ç§»é™¤çš„æ–‡ç« ä¿¡æ¯
    if not duplicates.empty:
        logging.info(f"\nğŸ“‹ å»é‡: ç§»é™¤ {len(duplicates)} æ¡é‡å¤æ–‡ç« ")
        for _, row in duplicates.iterrows():
            source = row.get('SourceCommonName', 'N/A')
            url = row.get('DocumentIdentifier', 'N/A')[:60]  # æˆªæ–­URL
            logging.info(f"   - [{source}] {url}...")
    
    # ç²¾ç¡®åŒ¹é…å»é‡ - ä¿ç•™ç¬¬ä¸€æ¡
    gkg_df = gkg_df.drop_duplicates(subset=['DocumentIdentifier'], keep='first')
    
    return gkg_df.reset_index(drop=True)


def _save_gkg_to_csv(gkg_df: pd.DataFrame, country_code: str = None, skip_dedup: bool = False) -> str:
    """ä¿å­˜ GKG DataFrame åˆ° CSV æ–‡ä»¶
    
    Args:
        gkg_df: GKG DataFrame
        country_code: å›½å®¶ä»£ç 
        skip_dedup: æ˜¯å¦è·³è¿‡å»é‡ï¼ˆå¦‚æœå¤–éƒ¨å·²å»é‡åˆ™è®¾ä¸º Trueï¼‰
    """
    os.makedirs(_GDELT_DATA_DIR, exist_ok=True)
    
    # å»é‡ï¼šåŸºäºæ ‡é¢˜å»é™¤ç›¸ä¼¼æ–‡ç« ï¼ˆå¦‚æœæœªè·³è¿‡ï¼‰
    if not skip_dedup:
        gkg_df = _deduplicate_by_url(gkg_df)
    
    if country_code:
        filename = f"{country_code.upper()}_gkg.csv"
    else:
        filename = "default_gkg.csv"
    
    file_path = os.path.join(_GDELT_DATA_DIR, filename)
    gkg_df.to_csv(file_path, index=False, encoding='utf-8-sig')
    logging.info(f"âœ“ GKG æ•°æ®å·²ä¿å­˜: {filename} ({len(gkg_df)} æ¡)")
    
    return file_path



def _save_events_to_csv(events, country_code: str = None) -> str:
    """ä¿å­˜ EventModel åˆ—è¡¨åˆ° CSV æ–‡ä»¶ï¼ˆä½¿ç”¨ BigQuery åˆ—åä»¥ä¾¿å¤ç”¨åŠ è½½å‡½æ•°ï¼‰"""
    os.makedirs(_GDELT_DATA_DIR, exist_ok=True)
    
    if country_code:
        filename = f"{country_code.upper()}_event.csv"
    else:
        filename = "default_event.csv"
    
    # å°† EventModel è½¬æ¢ä¸º DataFrameï¼ˆä½¿ç”¨ BigQuery åŸå§‹åˆ—åï¼‰
    rows = []
    for e in events:
        rows.append({
            'GLOBALEVENTID': e.global_event_id,
            'SQLDATE': e.sql_date,
            'Actor1Code': e.actor1.code,
            'Actor1Name': e.actor1.name,
            'Actor1CountryCode': e.actor1.country_code,
            'Actor1Type1Code': e.actor1.type1_code,
            'Actor2Code': e.actor2.code,
            'Actor2Name': e.actor2.name,
            'Actor2CountryCode': e.actor2.country_code,
            'Actor2Type1Code': e.actor2.type1_code,
            'EventCode': e.event_code,
            'EventBaseCode': e.event_base_code,
            'EventRootCode': e.event_root_code,
            'QuadClass': e.quad_class,
            'GoldsteinScale': e.goldstein_scale,
            'NumMentions': e.num_mentions,
            'NumSources': e.num_sources,
            'NumArticles': e.num_articles,
            'AvgTone': e.avg_tone,
            'ActionGeo_Type': e.action_geo.geo_type,
            'ActionGeo_FullName': e.action_geo.full_name,
            'ActionGeo_CountryCode': e.action_geo.country_code,
            'ActionGeo_ADM1Code': e.action_geo.adm1_code,
            'ActionGeo_Lat': e.action_geo.lat,
            'ActionGeo_Long': e.action_geo.long,
            'ActionGeo_FeatureID': e.action_geo.feature_id,
            'SOURCEURL': e.source_url,
            'DATEADDED': e.date_added,
        })
    
    df = pd.DataFrame(rows)
    file_path = os.path.join(_GDELT_DATA_DIR, filename)
    df.to_csv(file_path, index=False, encoding='utf-8-sig')
    logging.info(f"âœ“ Event æ•°æ®å·²ä¿å­˜: {filename} ({len(rows)} æ¡)")
    
    return file_path



def _print_event_summary(events, mentions):
    """æ‰“å°äº‹ä»¶æ±‡æ€»ä¿¡æ¯"""
    events_dict = {e.global_event_id: e for e in events}
    mentions_by_event = defaultdict(list)
    
    for mention in mentions:
        mentions_by_event[mention.global_event_id].append(mention)
    
    logging.info(f"\nğŸ“Š {len(mentions)} æ¡æŠ¥é“æŒ‰äº‹ä»¶åˆ†ç»„ï¼š")
    for event_id, event_mentions in mentions_by_event.items():
        event = events_dict.get(event_id)
        if event:
            logging.info(f"   EventID {event_id} | "
                  f"QuadClass={event.quad_class} | "
                  f"EventCode={event.event_code} | "
                  f"{event.action_geo.full_name} | "
                  f"{event.actor1.name or event.actor1.code} â†’ "
                  f"{event.actor2.name or event.actor2.code} | "
                  f"{len(event_mentions)} æ¡")
