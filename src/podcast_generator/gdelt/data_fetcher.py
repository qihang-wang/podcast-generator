"""
GDELT æ•°æ®è·å–æ¨¡å—
ä» BigQuery è·å– Event -> Mentions -> GKG å®Œæ•´æ•°æ®æµ

å…¬å¼€æ–¹æ³•ï¼š
    - fetch_gdelt_data: è·å– GDELT æ•°æ®çš„å”¯ä¸€å…¥å£
    - fetch_gkg_data: ç›´æ¥è·å– GKG æ•°æ®
"""

import os
import logging
import pandas as pd
import re
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Any, Optional

from .gdelt_service import GDELTQueryService
from .gdelt_mentions import select_best_mentions_per_event


# ========== ç§æœ‰å¸¸é‡ ==========
_GDELT_CSV_DIR = os.path.join(os.path.dirname(__file__), "gdelt_csv")


def _extract_timestamp_from_gkg_record_id(gkg_record_id: str) -> Optional[int]:
    """
    ä» gkg_record_id æå–æ—¶é—´æˆ³ï¼ˆæ–‡ç« å‘å¸ƒæ—¶é—´ï¼‰
    
    gkg_record_id æ ¼å¼: YYYYMMDDHHMMSS-XXXX-XXXXX...
    ä¾‹å¦‚: 20260122143045-T2-2-1-...
    
    Args:
        gkg_record_id: GKGè®°å½•ID
        
    Returns:
        YYYYMMDDHHMMSS æ ¼å¼çš„æ•´æ•°æ—¶é—´æˆ³ï¼Œè§£æå¤±è´¥è¿”å› None
    """
    if not gkg_record_id:
        return None
    
    # æå–å‰14ä½æ•°å­—ä½œä¸ºæ—¶é—´æˆ³
    match = re.match(r'^(\d{14})', str(gkg_record_id))
    if match:
        try:
            return int(match.group(1))
        except ValueError:
            return None
    return None


def fetch_gdelt_data(
    location_name: str = None,
    country_code: str = None,
    hours_back: int = None,
    start_time: datetime = None,
    end_time: datetime = None,
    event_limit: int = 100,
    min_confidence: int = 80,
    max_sentence_id: int = 1
):
    """
    è·å– GDELT å®Œæ•´æ•°æ®å¹¶ä¿å­˜åˆ° CSV æ–‡ä»¶
    
    è¿™æ˜¯è·å– GDELT æ•°æ®çš„å”¯ä¸€å…¬å¼€å…¥å£ã€‚è·å–å®Œæˆåè‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ° CSVã€‚
    
    Args:
        location_name: åœ°ç‚¹åç§°ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
        country_code: å›½å®¶ä»£ç ï¼ˆå¦‚ "CH" è¡¨ç¤ºä¸­å›½ï¼‰
        hours_back: æŸ¥è¯¢æœ€è¿‘Nå°æ—¶çš„æ•°æ®ï¼ˆä¸ start_time/end_time äºŒé€‰ä¸€ï¼‰
        start_time: å¼€å§‹æ—¶é—´ï¼ˆç²¾ç¡®æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰
        end_time: ç»“æŸæ—¶é—´ï¼ˆç²¾ç¡®æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰
        event_limit: äº‹ä»¶æ•°é‡é™åˆ¶ï¼Œé»˜è®¤100
        min_confidence: Mentions æœ€å°ç½®ä¿¡åº¦ï¼Œé»˜è®¤80%
        max_sentence_id: å¥å­IDé™åˆ¶ï¼ˆ1=ä»…å¯¼è¯­ï¼‰ï¼Œé»˜è®¤1
        
    Examples:
        # è·å–ä¸­å›½ç›¸å…³äº‹ä»¶ï¼ˆæœ€è¿‘24å°æ—¶ï¼‰
        fetch_gdelt_data(country_code="CH")
        
        # è·å–åŒ—äº¬ç›¸å…³äº‹ä»¶
        fetch_gdelt_data(location_name="Beijing")
        
        # è·å–ä¸­å›½æŸå¤©çš„äº‹ä»¶ï¼ˆç²¾ç¡®æ—¶é—´èŒƒå›´ï¼‰
        fetch_gdelt_data(
            country_code="CH",
            start_time=datetime(2026, 1, 21, 0, 0, 0),
            end_time=datetime(2026, 1, 21, 23, 59, 59)
        )
    """
    logging.info("\n" + "=" * 80)
    logging.info("ğŸš€ å¼€å§‹ GDELT æ•°æ®è·å–")
    logging.info("=" * 80)
    
    service = GDELTQueryService()
    
    # Step 1: è·å–äº‹ä»¶
    logging.info(f"\nğŸ“ æ­¥éª¤ 1/3: æŸ¥è¯¢ Event è¡¨")
    
    # æ‰“å°å‚æ•°ä¿¡æ¯
    if start_time and end_time:
        logging.info(f"   å‚æ•°: location={location_name or 'ä¸é™'}, country={country_code or 'ä¸é™'}")
        logging.info(f"   æ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%Y-%m-%d %H:%M')}")
    else:
        logging.info(f"   å‚æ•°: location={location_name or 'ä¸é™'}, country={country_code or 'ä¸é™'}, hours={hours_back or 24}h")
    
    events = service.query_events_by_location(
        location_name=location_name,
        country_code=country_code,
        hours_back=hours_back,
        start_time=start_time,
        end_time=end_time,
        limit=event_limit,
        print_progress=True
    )
    
    if not events:
        logging.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº‹ä»¶")
        return
    
    logging.info(f"âœ“ æ‰¾åˆ° {len(events)} ä¸ªäº‹ä»¶")
    
    # Step 2: è·å– Mentions
    logging.info(f"\nğŸ“° æ­¥éª¤ 2/3: æŸ¥è¯¢ Mentions è¡¨")
    logging.info(f"   å‚æ•°: Confidence>={min_confidence}%, SentenceID<={max_sentence_id}")
    
    event_ids = [e.global_event_id for e in events]
    all_mentions = service.query_mentions_by_event_ids(
        event_ids=event_ids,
        min_confidence=min_confidence,
        sentence_id=max_sentence_id,
        print_progress=True
    )
    
    if not all_mentions:
        logging.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æŠ¥é“")
        return
    
    # æ‰“å°äº‹ä»¶æ±‡æ€»
    _print_event_summary(events, all_mentions)
    
    # ç­›é€‰æœ€ä½³æŠ¥é“
    all_mentions = select_best_mentions_per_event(all_mentions)
    
    # ç­›é€‰å‡ºä¸ mentions ç›¸å…³çš„äº‹ä»¶
    related_event_ids = set(m.global_event_id for m in all_mentions)
    related_events = [e for e in events if e.global_event_id in related_event_ids]
    logging.info(f"âœ“ ç­›é€‰å‡º {len(related_events)} ä¸ªç›¸å…³äº‹ä»¶")
    
    # Step 3: è·å– GKG æ•°æ®
    logging.info(f"\nğŸ” æ­¥éª¤ 3/3: æŸ¥è¯¢ GKG è¡¨")
    
    mention_urls = [m.mention_identifier for m in all_mentions if m.mention_identifier]
    if not mention_urls:
        logging.warning("âš ï¸ æ— æœ‰æ•ˆ URL")
        return
    
    gkg_df = service.query_gkg_raw(mention_urls, print_progress=True)
    
    if gkg_df.empty:
        logging.warning("âš ï¸ æœªè·å–åˆ° GKG æ•°æ®")
        return
    
    # å»ºç«‹ URL -> EventID æ˜ å°„ï¼Œæ·»åŠ åˆ° GKG DataFrame
    url_to_event = {m.mention_identifier: m.global_event_id for m in all_mentions}
    gkg_df['event_id'] = gkg_df['DocumentIdentifier'].map(url_to_event)
    
    logging.info(f"âœ“ è·å–åˆ° {len(gkg_df)} æ¡ GKG æ•°æ®ï¼Œå·²å…³è” event_id")
    
    # ä¿å­˜åˆ° CSV
    _save_gkg_to_csv(gkg_df, country_code)
    _save_events_to_csv(related_events, country_code)
    
    # åŒæ­¥åˆ°æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    _sync_to_supabase(gkg_df, country_code)

    
    # å®Œæˆ
    logging.info("\n" + "=" * 80)
    logging.info(f"âœ… å®Œæˆï¼{len(related_events)} ä¸ªäº‹ä»¶ï¼Œ{len(gkg_df)} ç¯‡æ–‡ç« ")
    logging.info("=" * 80 + "\n")


def fetch_gkg_data(
    country_code: str,
    hours_back: int = None,
    start_time: datetime = None,
    end_time: datetime = None,
    themes: list = None,
    allowed_languages: list = None,
    min_word_count: int = 200,
    limit: int = 20
):
    """
    ç›´æ¥é€šè¿‡å›½å®¶ä»£ç è·å– GKG æ•°æ®å¹¶ä¿å­˜åˆ° CSV æ–‡ä»¶
    
    è·³è¿‡ Event å’Œ Mentions æŸ¥è¯¢æ­¥éª¤ï¼Œç›´æ¥ä» GKG è¡¨æŒ‰å›½å®¶æŸ¥è¯¢ã€‚
    é€‚ç”¨äºå¿«é€Ÿè·å–æŸä¸ªå›½å®¶/åŒºåŸŸçš„çƒ­ç‚¹æ–°é—»æ–‡ç« åˆ†ææ•°æ®ã€‚
    
    Args:
        country_code: FIPS å›½å®¶ä»£ç ï¼Œå¦‚ "US", "CH"(ä¸­å›½), "UK", "JP" ç­‰
        hours_back: æŸ¥è¯¢æœ€è¿‘Nå°æ—¶çš„æ•°æ®ï¼ˆä¸ start_time/end_time äºŒé€‰ä¸€ï¼‰
        start_time: å¼€å§‹æ—¶é—´ï¼ˆç²¾ç¡®æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰
        end_time: ç»“æŸæ—¶é—´ï¼ˆç²¾ç¡®æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰
        themes: ä¸»é¢˜è¿‡æ»¤åˆ—è¡¨ï¼Œå¦‚ ["PROTESTS", "ELECTIONS"]ï¼Œé»˜è®¤Noneä¸è¿‡æ»¤
        allowed_languages: å…è®¸çš„è¯­è¨€ä»£ç åˆ—è¡¨ï¼Œå¦‚ ['eng', 'zho']
                          é»˜è®¤Noneä½¿ç”¨é¢„è®¾çš„ä¸»æµè¯­è¨€åˆ—è¡¨
        min_word_count: æœ€å°å­—æ•°è¿‡æ»¤ï¼Œé»˜è®¤100
        limit: è¿”å›æ•°é‡é™åˆ¶ï¼Œé»˜è®¤100
        
    Returns:
        pandas.DataFrame: GKG åŸå§‹æ•°æ®
        
    Examples:
        # è·å–ç¾å›½æœ€è¿‘24å°æ—¶çš„æ–°é—»
        df = fetch_gkg_data("US")
        
        # è·å–ä¸­å›½æœ€è¿‘12å°æ—¶å…³äºæŠ—è®®çš„æ–°é—»
        df = fetch_gkg_data("CH", hours_back=12, themes=["PROTESTS"])
        
        # è·å–æ—¥æœ¬æŸå¤©çš„æ–°é—»ï¼ˆç²¾ç¡®æ—¶é—´èŒƒå›´ï¼‰
        df = fetch_gkg_data(
            "JA",
            start_time=datetime(2026, 1, 21, 0, 0, 0),
            end_time=datetime(2026, 1, 21, 23, 59, 59)
        )
    """
    logging.info("\n" + "=" * 80)
    logging.info("ğŸš€ å¼€å§‹ GKG æ•°æ®ç›´æ¥è·å–")
    logging.info("=" * 80)
    
    # æ‰“å°å‚æ•°ä¿¡æ¯
    if start_time and end_time:
        logging.info(f"\nğŸ“ å‚æ•°: country={country_code}, æ—¶é—´èŒƒå›´={start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%Y-%m-%d %H:%M')}, limit={limit}")
    else:
        logging.info(f"\nğŸ“ å‚æ•°: country={country_code}, hours={hours_back or 24}h, limit={limit}")
    
    if themes:
        logging.info(f"   ä¸»é¢˜è¿‡æ»¤: {themes}")
    if allowed_languages:
        logging.info(f"   è¯­è¨€è¿‡æ»¤: {allowed_languages}")
    
    service = GDELTQueryService()
    
    # ç›´æ¥æŸ¥è¯¢ GKG è¡¨
    logging.info(f"\nğŸ” æŸ¥è¯¢ GKG è¡¨...")
    gkg_df = service.query_gkg_by_country(
        country_code=country_code,
        hours_back=hours_back,
        start_time=start_time,
        end_time=end_time,
        themes=themes,
        allowed_languages=allowed_languages,
        min_word_count=min_word_count,
        limit=limit,
        print_progress=True
    )
    
    if gkg_df.empty:
        logging.warning("âš ï¸ æœªè·å–åˆ° GKG æ•°æ®")
        return
    
    logging.info(f"âœ“ è·å–åˆ° {len(gkg_df)} æ¡ GKG æ•°æ®")
    
    # ä¿å­˜åˆ° CSVï¼ˆå†…éƒ¨ä¼šæ‰§è¡Œå»é‡ï¼‰
    _save_gkg_to_csv(gkg_df, country_code)
    
    # åŒæ­¥åˆ°æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    _sync_to_supabase(gkg_df, country_code)
    
    # å®Œæˆ
    logging.info("\n" + "=" * 80)
    logging.info(f"âœ… å®Œæˆï¼{len(gkg_df)} ç¯‡æ–‡ç« ")
    logging.info("=" * 80 + "\n")




# ========== æ•°æ®åº“åŒæ­¥ ==========

def _sync_to_supabase(gkg_df: pd.DataFrame, country_code: str):
    """
    å°† GKG æ•°æ®åŒæ­¥åˆ° Supabaseï¼ˆæŒ‰æ—¶é—´æ’åºå­˜å‚¨ï¼‰
    
    ä»…åœ¨ ENABLE_DATABASE_SYNC=true æ—¶æ‰§è¡Œ
    """
    try:
        from podcast_generator.database import ArticleRepository
        from podcast_generator.gdelt.gdelt_parse import parse_gdelt_article
        from .gdelt_gkg import _row_to_gkg_model
        
        repo = ArticleRepository()
        
        if not repo.is_sync_enabled():
            logging.debug("æ•°æ®åº“åŒæ­¥æœªå¯ç”¨ï¼Œè·³è¿‡")
            return
        
        logging.info("\nğŸ“¤ åŒæ­¥æ•°æ®åˆ° Supabase...")
        
        records = []
        for _, row in gkg_df.iterrows():
            gkg = _row_to_gkg_model(row)
            params = parse_gdelt_article(gkg, event=None, fetch_content=False)
            
            # ä» gkg_record_id æå–æ–‡ç« å‘å¸ƒæ—¶é—´ï¼ˆæ›´ç²¾ç¡®ï¼‰
            published_at = _extract_timestamp_from_gkg_record_id(gkg.gkg_record_id)
            
            record = {
                "country_code": country_code.upper() if country_code else "UNKNOWN",
                "gkg_record_id": gkg.gkg_record_id,
                "date_added": gkg.date,  # GDELT æ‰¹æ¬¡æ—¶é—´æˆ³ï¼ˆç”¨äºæŸ¥è¯¢è¿‡æ»¤ï¼Œä¸ BigQuery ä¸€è‡´ï¼‰
                "published_at": published_at,  # æ–‡ç« å‘å¸ƒæ—¶é—´ï¼ˆä» gkg_record_id æå–ï¼Œæ›´ç²¾ç¡®ï¼‰
                "title": params.get("title"),
                "source": params.get("source"),
                "url": params.get("url"),
                "authors": params.get("authors"),
                "persons": params.get("persons", []),
                "organizations": params.get("organizations", []),
                "themes": params.get("themes", []),
                "locations": params.get("locations", []),
                "quotations": params.get("quotations", []),
                "amounts": params.get("amounts", []),
                "tone": params.get("tone"),
                "emotion": params.get("emotion"),
                "emotion_instruction": params.get("emotion_instruction"),
                "event": params.get("event"),
                "images": params.get("images", []),
                "videos": params.get("videos", []),
            }
            records.append(record)
        
        # æ‰¹é‡æ’å…¥ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
        count = repo.bulk_upsert(records)
        logging.info(f"âœ… å·²åŒæ­¥ {count} æ¡æ•°æ®åˆ° Supabase")
        
    except ImportError as e:
        logging.debug(f"æ•°æ®åº“æ¨¡å—æœªå®‰è£…: {e}")
    except Exception as e:
        logging.error(f"âŒ Supabase åŒæ­¥å¤±è´¥: {e}")


# ========== ç§æœ‰æ–¹æ³• ==========

def _deduplicate_by_title(gkg_df: pd.DataFrame) -> pd.DataFrame:
    """
    åŸºäºæ ‡é¢˜å»é‡ï¼Œç§»é™¤ç›¸ä¼¼æ–‡ç« 
    
    åŒä¸€é€šè®¯ç¤¾ç¨¿ä»¶ï¼ˆå¦‚AFP/Reutersï¼‰ç»å¸¸è¢«å¤šå®¶åª’ä½“è½¬è½½ï¼Œ
    å¯¼è‡´ GKG ä¸­å‡ºç°å¤šæ¡ç›¸åŒå†…å®¹çš„è®°å½•ã€‚
    """
    if 'Article_Title' not in gkg_df.columns:
        return gkg_df
    
    # æ¸…ç†æ ‡é¢˜ï¼šè½¬å°å†™ã€å»é™¤ç©ºç™½
    gkg_df['_clean_title'] = gkg_df['Article_Title'].fillna('').str.lower().str.strip()
    
    # è®°å½•åŸå§‹æ•°é‡
    original_count = len(gkg_df)
    
    # æ‰¾å‡ºé‡å¤çš„è®°å½•ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼Œæ ‡è®°å…¶ä½™ä¸ºé‡å¤ï¼‰
    duplicates = gkg_df[gkg_df.duplicated(subset=['_clean_title'], keep='first')]
    
    # æ‰“å°è¢«ç§»é™¤çš„æ–‡ç« ä¿¡æ¯
    if not duplicates.empty:
        logging.info(f"\nğŸ“‹ å»é‡: ç§»é™¤ {len(duplicates)} æ¡é‡å¤æ–‡ç« ")
        for _, row in duplicates.iterrows():
            title = row.get('Article_Title', 'N/A')[:50]  # æˆªæ–­æ ‡é¢˜
            source = row.get('SourceCommonName', 'N/A')
            url = row.get('DocumentIdentifier', 'N/A')[:60]  # æˆªæ–­URL
            logging.info(f"   - [{source}] {title}...")
            logging.info(f"     URL: {url}...")
    
    # ç²¾ç¡®åŒ¹é…å»é‡ - ä¿ç•™ç¬¬ä¸€æ¡
    gkg_df = gkg_df.drop_duplicates(subset=['_clean_title'], keep='first')
    
    # æ¸…ç†ä¸´æ—¶åˆ—
    gkg_df = gkg_df.drop(columns=['_clean_title'])
    
    return gkg_df.reset_index(drop=True)


def _save_gkg_to_csv(gkg_df: pd.DataFrame, country_code: str = None) -> str:
    """ä¿å­˜ GKG DataFrame åˆ° CSV æ–‡ä»¶ï¼ˆå†™å…¥å‰è‡ªåŠ¨å»é‡ï¼‰"""
    os.makedirs(_GDELT_CSV_DIR, exist_ok=True)
    
    # å»é‡ï¼šåŸºäºæ ‡é¢˜å»é™¤ç›¸ä¼¼æ–‡ç« ï¼ˆåŒä¸€é€šè®¯ç¤¾ç¨¿ä»¶è¢«å¤šå®¶åª’ä½“è½¬è½½ï¼‰
    gkg_df = _deduplicate_by_title(gkg_df)
    
    if country_code:
        filename = f"{country_code.upper()}_gkg.csv"
    else:
        filename = "default_gkg.csv"
    
    file_path = os.path.join(_GDELT_CSV_DIR, filename)
    gkg_df.to_csv(file_path, index=False, encoding='utf-8-sig')
    logging.info(f"âœ“ GKG æ•°æ®å·²ä¿å­˜: {filename} ({len(gkg_df)} æ¡)")
    
    return file_path



def _save_events_to_csv(events, country_code: str = None) -> str:
    """ä¿å­˜ EventModel åˆ—è¡¨åˆ° CSV æ–‡ä»¶ï¼ˆä½¿ç”¨ BigQuery åˆ—åä»¥ä¾¿å¤ç”¨åŠ è½½å‡½æ•°ï¼‰"""
    os.makedirs(_GDELT_CSV_DIR, exist_ok=True)
    
    if country_code:
        filename = f"{country_code.upper()}_event.csv"
    else:
        filename = "default_event.csv"
    
    # å°† EventModel è½¬æ¢ä¸º DataFrameï¼ˆä½¿ç”¨ BigQuery åŸå§‹åˆ—åï¼‰
    rows = []
    for e in events:
        rows.append({
            'GLOBALEVENTID': e.global_event_id,
            'SQLDATE': e.sql_date,
            'Actor1Code': e.actor1.code,
            'Actor1Name': e.actor1.name,
            'Actor1CountryCode': e.actor1.country_code,
            'Actor1Type1Code': e.actor1.type1_code,
            'Actor2Code': e.actor2.code,
            'Actor2Name': e.actor2.name,
            'Actor2CountryCode': e.actor2.country_code,
            'Actor2Type1Code': e.actor2.type1_code,
            'EventCode': e.event_code,
            'EventBaseCode': e.event_base_code,
            'EventRootCode': e.event_root_code,
            'QuadClass': e.quad_class,
            'GoldsteinScale': e.goldstein_scale,
            'NumMentions': e.num_mentions,
            'NumSources': e.num_sources,
            'NumArticles': e.num_articles,
            'AvgTone': e.avg_tone,
            'ActionGeo_Type': e.action_geo.geo_type,
            'ActionGeo_FullName': e.action_geo.full_name,
            'ActionGeo_CountryCode': e.action_geo.country_code,
            'ActionGeo_ADM1Code': e.action_geo.adm1_code,
            'ActionGeo_Lat': e.action_geo.lat,
            'ActionGeo_Long': e.action_geo.long,
            'ActionGeo_FeatureID': e.action_geo.feature_id,
            'SOURCEURL': e.source_url,
            'DATEADDED': e.date_added,
        })
    
    df = pd.DataFrame(rows)
    file_path = os.path.join(_GDELT_CSV_DIR, filename)
    df.to_csv(file_path, index=False, encoding='utf-8-sig')
    logging.info(f"âœ“ Event æ•°æ®å·²ä¿å­˜: {filename} ({len(rows)} æ¡)")
    
    return file_path



def _print_event_summary(events, mentions):
    """æ‰“å°äº‹ä»¶æ±‡æ€»ä¿¡æ¯"""
    events_dict = {e.global_event_id: e for e in events}
    mentions_by_event = defaultdict(list)
    
    for mention in mentions:
        mentions_by_event[mention.global_event_id].append(mention)
    
    logging.info(f"\nğŸ“Š {len(mentions)} æ¡æŠ¥é“æŒ‰äº‹ä»¶åˆ†ç»„ï¼š")
    for event_id, event_mentions in mentions_by_event.items():
        event = events_dict.get(event_id)
        if event:
            logging.info(f"   EventID {event_id} | "
                  f"QuadClass={event.quad_class} | "
                  f"EventCode={event.event_code} | "
                  f"{event.action_geo.full_name} | "
                  f"{event.actor1.name or event.actor1.code} â†’ "
                  f"{event.actor2.name or event.actor2.code} | "
                  f"{len(event_mentions)} æ¡")
