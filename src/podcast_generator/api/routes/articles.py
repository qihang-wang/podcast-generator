"""
æ–‡ç« æ•°æ® API è·¯ç”±
æ”¯æŒä» CSV æˆ– Supabase è·å–æ•°æ®
"""

from fastapi import APIRouter, Query, HTTPException
from typing import Optional
from datetime import datetime, timedelta
import logging

router = APIRouter(prefix="/api/articles", tags=["æ–‡ç« æ•°æ®"])


def _get_time_range(days: int):
    """è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆè¿”å› YYYYMMDDHHMMSS æ ¼å¼çš„æ•´æ•°ï¼‰"""
    now = datetime.now()
    end_time = int(now.strftime("%Y%m%d%H%M%S"))
    start_time = int((now - timedelta(days=days)).strftime("%Y%m%d%H%M%S"))
    return start_time, end_time


@router.get("/")
async def get_articles(
    country_code: str = Query("CH", description="å›½å®¶ä»£ç  (FIPS 10-4)"),
    days: int = Query(1, ge=1, le=7, description="è·å–æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆ1-7å¤©ï¼‰"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡")
):
    """
    è·å–æŒ‡å®šå›½å®¶çš„æ–‡ç« æ•°æ®
    
    æ•°æ®æºï¼šSupabase æ•°æ®åº“ï¼ˆCSV ä»…ä½œä¸ºå†™å…¥ç¼“å­˜ï¼‰
    
    å‚æ•°ï¼š
    - **country_code**: å›½å®¶ä»£ç ï¼Œå¦‚ "CH"=ä¸­å›½, "US"=ç¾å›½
    - **days**: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆ1-7å¤©ï¼‰
    - **page**: é¡µç 
    - **page_size**: æ¯é¡µæ•°é‡
    """
    try:
        # ä»æ•°æ®åº“è·å–æ•°æ®
        result = await _get_from_database(country_code, days, page, page_size)
        
        return {
            "success": True,
            "source": "database",
            "cache_hit": result.get("cache_hit", False),
            "data": result["data"],
            "total": result["total"],
            "page": result["page"],
            "page_size": result["page_size"],
            "total_pages": (result["total"] + page_size - 1) // page_size if result["total"] > 0 else 0
        }
    
    except Exception as e:
        logging.error(f"è·å–æ–‡ç« å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
        )


async def _get_from_database(
    country_code: str, 
    days: int, 
    page: int, 
    page_size: int
) -> dict:
    """ä» Supabase æ•°æ®åº“è·å–æ•°æ®ï¼ˆå”¯ä¸€æ•°æ®æºï¼‰"""
    from podcast_generator.database import ArticleRepository
    
    repo = ArticleRepository()
    
    if not repo.is_available():
        raise HTTPException(
            status_code=503,
            detail="æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ Supabase é…ç½®"
        )
    
    start_time, end_time = _get_time_range(days)
    
    # æ£€æŸ¥ç¼“å­˜è¦†ç›–æƒ…å†µ
    coverage = repo.check_cache_coverage(country_code, start_time, end_time)
    
    if not coverage["covered"]:
        # ç¼“å­˜æœªå®Œå…¨è¦†ç›–ï¼Œéœ€è¦å¢é‡è·å–
        if not coverage["has_data"]:
            # å®Œå…¨æ²¡æ•°æ®ï¼Œè·å–æ•´ä¸ªæ—¶é—´èŒƒå›´
            logging.info(f"ğŸ“¥ æ•°æ®åº“æ— æ•°æ®ï¼Œè·å–å®Œæ•´ {days} å¤©æ•°æ®")
            await _fetch_missing_data(country_code, days)
        else:
            # éƒ¨åˆ†æ•°æ®ç¼ºå¤±ï¼Œåªè·å–ç¼ºå¤±çš„éƒ¨åˆ†
            logging.info(f"ğŸ“¥ æ•°æ®éƒ¨åˆ†ç¼ºå¤±ï¼Œå¢é‡è·å–ç¼ºå¤±æ—¶æ®µ")
            
            # è®¡ç®—ç¼ºå¤±çš„æ—¶é—´æ®µå¹¶è·å–
            if coverage["missing_before"]:
                # éœ€è¦è·å–æ›´æ—©çš„æ•°æ®
                missing_days_before = _calculate_days_diff(
                    start_time, 
                    coverage["cached_range"][0]
                )
                if missing_days_before > 0:
                    logging.info(f"  â†’ è·å–å‰é¢ç¼ºå¤±çš„ {missing_days_before} å¤©æ•°æ®")
                    await _fetch_time_range_data(
                        country_code, 
                        start_time, 
                        coverage["cached_range"][0]
                    )
            
            if coverage["missing_after"]:
                # éœ€è¦è·å–æ›´æ–°çš„æ•°æ®
                missing_days_after = _calculate_days_diff(
                    coverage["cached_range"][1],
                    end_time
                )
                if missing_days_after > 0:
                    logging.info(f"  â†’ è·å–åé¢ç¼ºå¤±çš„ {missing_days_after} å¤©æ•°æ®")
                    await _fetch_time_range_data(
                        country_code,
                        coverage["cached_range"][1],
                        end_time
                    )
    
    # æŸ¥è¯¢æ•°æ®ï¼ˆæ— è®ºæ˜¯å¦è·å–æˆåŠŸï¼Œéƒ½ä»æ•°æ®åº“è¿”å›ï¼‰
    result = repo.query_by_country_and_time(
        country_code, start_time, end_time, page, page_size
    )
    
    return {
        "cache_hit": coverage.get("covered", False),
        "data": result["data"],
        "total": result["total"],
        "page": result["page"],
        "page_size": result["page_size"]
    }


def _calculate_days_diff(start_time: int, end_time: int) -> int:
    """è®¡ç®—ä¸¤ä¸ªæ—¶é—´æˆ³ä¹‹é—´çš„å¤©æ•°å·®"""
    try:
        start_dt = datetime.strptime(str(start_time), "%Y%m%d%H%M%S")
        end_dt = datetime.strptime(str(end_time), "%Y%m%d%H%M%S")
        diff = abs((end_dt - start_dt).days)
        return max(1, diff)  # è‡³å°‘è¿”å›1å¤©
    except:
        return 1


async def _fetch_time_range_data(country_code: str, start_time: int, end_time: int):
    """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®ï¼ˆå¢é‡è·å–ï¼‰"""
    try:
        from podcast_generator.gdelt.data_fetcher import fetch_gkg_data
        
        # è®¡ç®—éœ€è¦è·å–å¤šå°‘å°æ—¶çš„æ•°æ®
        days = _calculate_days_diff(start_time, end_time)
        hours_back = days * 24
        
        logging.info(f"   ä» BigQuery è·å– {country_code} æ—¶é—´èŒƒå›´: {start_time} - {end_time} ({days}å¤©)")
        
        # è·å–æ•°æ®ï¼ˆä¼šè‡ªåŠ¨åŒæ­¥åˆ°æ•°æ®åº“ï¼‰
        fetch_gkg_data(
            country_code=country_code,
            hours_back=hours_back,
            limit=100
        )
    except Exception as e:
        logging.error(f"è·å–æ—¶é—´èŒƒå›´æ•°æ®å¤±è´¥: {e}")


async def _fetch_missing_data(country_code: str, days: int):
    """è·å–ç¼ºå¤±çš„æ•°æ®ï¼ˆä» BigQueryï¼‰- å®Œæ•´æ—¶é—´èŒƒå›´"""
    try:
        from podcast_generator.gdelt.data_fetcher import fetch_gkg_data
        
        hours_back = days * 24
        logging.info(f"   ä» BigQuery è·å– {country_code} æœ€è¿‘ {hours_back} å°æ—¶æ•°æ®...")
        
        # è¿™ä¼šä¿å­˜åˆ° CSV å¹¶åŒæ­¥åˆ°æ•°æ®åº“
        fetch_gkg_data(
            country_code=country_code,
            hours_back=hours_back,
            limit=100
        )
    except Exception as e:
        logging.error(f"è·å–æ•°æ®å¤±è´¥: {e}")



async def _get_from_csv(country_code: str, fetch_content: bool) -> dict:
    """ä» CSV æ–‡ä»¶è·å–æ•°æ®"""
    from podcast_generator.gdelt.data_loader import load_gdelt_data
    from podcast_generator.gdelt.gdelt_parse import parse_gdelt_article
    
    # åŠ è½½æ•°æ®
    gkg_models, event_models = load_gdelt_data(country_code=country_code)
    
    # å»ºç«‹ Event æ˜ å°„
    events_dict = {e.global_event_id: e for e in event_models}
    
    # è§£ææ¯ç¯‡æ–‡ç« 
    articles = []
    for gkg in gkg_models:
        event = events_dict.get(gkg.event_id)
        params = parse_gdelt_article(gkg, event, fetch_content=fetch_content)
        articles.append(params)
    
    return {
        "data": articles,
        "total": len(articles)
    }


@router.get("/stats")
async def get_stats():
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            return {
                "success": True,
                "database_available": False,
                "message": "æ•°æ®åº“æœªé…ç½®"
            }
        
        total = repo.get_article_count()
        
        return {
            "success": True,
            "database_available": True,
            "total_articles": total
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


@router.post("/cleanup")
async def cleanup_old_articles(days: int = Query(7, description="ä¿ç•™æœ€è¿‘Nå¤©çš„æ•°æ®")):
    """æ¸…ç†è¿‡æœŸæ•°æ®"""
    try:
        from podcast_generator.database import ArticleRepository
        
        repo = ArticleRepository()
        
        if not repo.is_available():
            raise HTTPException(status_code=503, detail="æ•°æ®åº“æœªé…ç½®")
        
        deleted = repo.cleanup_old_articles(days=days)
        
        return {
            "success": True,
            "deleted_count": deleted,
            "message": f"å·²æ¸…ç† {deleted} æ¡è¶…è¿‡ {days} å¤©çš„æ•°æ®"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
